% ------------------------------------------------
\StartChapter{Experimental Setup}{chapter:experimental-setup}
% ------------------------------------------------

This chapter presents the comprehensive experimental methodology designed to rigorously evaluate the GemGNN framework's core innovations in heterogeneous graph construction, multi-view learning, and few-shot fake news detection. Our experimental design emphasizes the authenticity of evaluation protocols and the logical validation of each architectural component's contribution to the overall system performance.

\section{Dataset Selection and Justification}

\subsection{FakeNewsNet Benchmark Datasets}

We conduct experiments on the FakeNewsNet benchmark \cite{shu2018fakenewsnet}, specifically utilizing the PolitiFact and GossipCop datasets. These datasets are selected not merely for their widespread adoption, but for their fundamental suitability to validate our approach's core hypotheses about content-based fake news detection in few-shot scenarios.

\subsubsection{PolitiFact: Political Misinformation Detection}

\textbf{Dataset Rationale:} Political news provides an ideal testbed for our content-based approach because political misinformation often contains subtle factual distortions embedded within otherwise accurate information. This characteristic allows us to evaluate whether our heterogeneous graph structure can capture nuanced semantic relationships that distinguish genuine from manipulated political content.

\textbf{Statistical Distribution:} 
\begin{itemize}
\item Training set: 246 real, 135 fake articles (381 total; 64.6\% real)
\item Test set: 73 real, 29 fake articles (102 total; 71.6\% real)  
\item Complete dataset: 319 real, 164 fake articles (483 total; 66.0\% real)
\end{itemize}

\textbf{Content Complexity:} Political articles in this dataset typically range from 200-800 words and contain factual assertions that can be independently verified. The class imbalance (2:1 real-to-fake ratio) reflects realistic deployment scenarios where legitimate news outnumbers fabricated content, making this dataset particularly suitable for evaluating few-shot performance under realistic conditions.

\subsubsection{GossipCop: Entertainment Content Validation}

\textbf{Dataset Rationale:} Entertainment news presents fundamentally different linguistic patterns and verification challenges compared to political content. Celebrity and entertainment articles often involve subjective interpretations, speculation, and sensational language, providing a complementary evaluation domain that tests our approach's generalization capabilities across content types.

\textbf{Statistical Distribution:}
\begin{itemize}
\item Training set: 7,955 real, 2,033 fake articles (9,988 total; 79.6\% real)  
\item Test set: 2,169 real, 503 fake articles (2,672 total; 81.2\% real)
\item Complete dataset: 10,124 real, 2,536 fake articles (12,660 total; 79.9\% real)
\end{itemize}

\textbf{Content Characteristics:} Entertainment articles typically exhibit more varied linguistic styles, emotional language, and speculative content compared to political news. The 4:1 real-to-fake ratio provides a different class balance that tests our framework's robustness to varying data distributions, while the larger dataset size (26x larger than PolitiFact) enables more comprehensive statistical analysis.

\subsection{Evaluation Protocol Authenticity}

\textbf{Content-Only Constraint:} Our experimental design explicitly focuses on content-based detection without relying on social propagation data, user behavior patterns, or network metadata. This constraint is not merely a limitation but a strategic design choice that ensures our approach remains applicable in scenarios where privacy regulations, platform restrictions, or real-time deployment requirements prevent access to social data.

\textbf{Professional Verification Standard:} Both datasets utilize professional fact-checker verification, providing high-confidence ground truth labels essential for reliable few-shot evaluation. The professional verification process ensures that our experimental results reflect genuine detection capability rather than biases in crowd-sourced or automated labeling.

\section{Core Architecture Components}

\subsection{DeBERTa Embedding Foundation}

\textbf{Architecture Selection Rationale:} We select DeBERTa (Decoding-enhanced BERT with Disentangled Attention) as our embedding foundation based on its unique architectural properties that enable effective multi-view learning. Unlike traditional transformers, DeBERTa's disentangled attention mechanism separates content and position representations, creating embeddings with superior partitioning characteristics essential for our multi-view approach.

\textbf{Embedding Generation Process:} Each news article undergoes processing through DeBERTa-base to generate 768-dimensional embeddings using the [CLS] token representation. This global document embedding captures comprehensive semantic information while maintaining the disentangled properties necessary for meaningful dimension partitioning in our multi-view construction.

\textbf{Multi-View Partitioning Strategy:} The 768-dimensional DeBERTa embeddings are systematically partitioned into multiple views (typically 3 views of 256 dimensions each), where each partition captures distinct semantic aspects of the content. This partitioning strategy leverages DeBERTa's internal attention structure to ensure that each view maintains discriminative power while focusing on different linguistic and semantic dimensions.

\subsection{Graph Construction and Architecture}

\textbf{Graph Structure:} Our approach constructs heterogeneous graphs with news nodes and interaction nodes to capture both content semantics and social patterns within a unified structure.

\textbf{Synthetic Interaction Generation:} For each news article, we generate 20 synthetic user interactions using large language models, distributed across three distinct tones: 8 neutral (factual focus), 7 affirmative (supportive), and 5 skeptical (questioning) interactions.

\textbf{Edge Construction:} We implement KNN-based edge construction approaches with test-isolated connectivity to prevent information leakage during evaluation.

\subsection{Neural Network Architecture}

\textbf{Heterogeneous Attention Networks (HAN):} We employ HAN architecture for processing heterogeneous graph structures through hierarchical attention mechanisms. HAN operates at node-level and semantic-level attention for information aggregation.

\textbf{Loss Function:} We employ cross-entropy loss with label smoothing as our training objective, using a smoothing factor of 0.1 for regularization in few-shot scenarios.

\section{Baseline Methods and Comparative Framework}

\subsection{Baseline Selection Strategy}

Our baseline selection follows a systematic approach to cover the full spectrum of fake news detection methodologies, enabling comprehensive evaluation of our approach's innovations across different paradigms.

\textbf{Traditional Content-Based Methods:}
\begin{itemize}
\item \textbf{Multi-Layer Perceptron (MLP):} Uses DeBERTa embeddings as static features for binary classification (hidden layers: 256, 128 units; ReLU activation; dropout: 0.3). Establishes performance baseline for content-only classification without structural information.

\item \textbf{Bidirectional LSTM:} Processes articles as word sequences with 128 hidden units. Tests whether sequential modeling provides advantages over static embeddings for fake news detection.
\end{itemize}

\textbf{Transformer-Based Language Models:}
\begin{itemize}
\item \textbf{BERT-base-uncased:} Fine-tuned for binary classification using [CLS] token representation (learning rate: 2e-5; batch size: 16; max length: 512 tokens).

\item \textbf{RoBERTa-base:} Optimized BERT variant with improved training procedures, using identical hyperparameters for fair comparison.
\end{itemize}

\textbf{Large Language Models:}
\begin{itemize}
\item \textbf{LLaMA-7B:} Evaluated through in-context learning with 2-3 examples per class from support set.

\item \textbf{Gemma-7B:} Complementary LLM evaluation using identical prompt engineering strategies.
\end{itemize}

\textbf{Graph-Based Methods:}
\begin{itemize}
\item \textbf{Less4FD:} Recent graph-based approach using KNN similarity graphs with GCN message passing.

\item \textbf{HeteroSGT:} Heterogeneous graph method adapted for content-only setting by removing social features.
\end{itemize}

\section{Few-Shot Evaluation Methodology}

\subsection{K-Shot Learning Protocol}

\textbf{Shot Configuration Rationale:} We evaluate across K âˆˆ {3, 4, 8, 16} shots per class, spanning from extremely few-shot (3-shot) to moderate few-shot (16-shot) scenarios. This range captures realistic deployment scenarios where labeled examples are scarce while providing sufficient statistical power for meaningful comparison.

\textbf{Support Set Sampling Strategy:} For each K-shot experiment, we employ stratified random sampling to select K examples per class from the training set. The sampling process ensures balanced representation across both classes and, where possible, different temporal periods and subtopics to minimize selection bias.

\textbf{Transductive Learning Framework:} Our evaluation employs transductive learning where all nodes (labeled training, unlabeled training, and test) participate in graph construction and message passing, but loss computation is restricted to labeled nodes. This paradigm maximizes the utility of available data while maintaining proper evaluation boundaries.

\textbf{Statistical Robustness:} We conduct multiple independent experimental runs for each configuration using different random seeds for support set sampling. Performance is reported as mean values across runs, ensuring statistical validity in few-shot learning scenarios.

\subsection{Performance Metrics and Statistical Analysis}

\textbf{Primary Metric Selection:} We employ Macro F1-score as our primary evaluation metric due to the class imbalance present in both datasets (PolitiFact: 2:1 real-to-fake; GossipCop: 4:1 real-to-fake). Macro F1-score provides a balanced assessment of precision and recall across both classes, making it particularly suitable for imbalanced few-shot scenarios where overall accuracy may be misleading.

\textbf{Comprehensive Metric Suite:} We report accuracy, precision, recall, and F1-score to provide complete performance characterization. This multi-metric approach reveals whether models exhibit class-specific biases and enables detailed analysis of failure modes.

\textbf{Statistical Significance Testing:} We employ paired t-tests to assess statistical significance of performance differences, accounting for the paired nature of few-shot experiments where identical support sets are used across methods. Bonferroni correction is applied for multiple comparisons across K-shot settings and datasets (Î± = 0.05).

\textbf{Effect Size Quantification:} Beyond statistical significance, we report Cohen's d effect sizes to quantify the practical significance of performance differences, ensuring that reported improvements represent meaningful advances rather than merely statistically detectable differences.

\section{Implementation Details and Experimental Configuration}

\subsection{Hyperparameter Selection and Optimization}

\textbf{Graph Construction Parameters:}
\begin{itemize}
\item K-nearest neighbors: k âˆˆ \{3, 5, 7\}
\item Multi-view partitioning: Evaluated across \{0, 3, 6\} view configurations
\item Synthetic interaction distribution: 20 interactions per article (8 neutral, 7 affirmative, 5 skeptical)
\item Similarity metric: Cosine similarity for edge construction
\item Unlabeled sampling factor: 5Ã—
\end{itemize}

\textbf{Neural Network Architecture:}
\begin{itemize}
\item Hidden dimensions: 64 units in GNN layers (optimized from {32, 64, 128})
\item Attention heads: 4 heads for multi-head attention mechanisms
\item Network depth: 1 GNN layers (optimized from {1, 2, 3, 4})
\item Dropout rate: 0.3 for regularization (optimized from {0.1, 0.3, 0.5})
\item Activation function: ReLU throughout hidden layers
\end{itemize}

\textbf{Training Configuration:}
\begin{itemize}
\item Optimizer: Adam with learning rate 5e-4 (optimized from {1e-4, 5e-4, 1e-3})
\item Weight decay: 1e-3 for L2 regularization
\item Batch processing: Full graph training (transductive setting)
\item Maximum epochs: 300 with early stopping
\item Early stopping patience: 30 epochs
\item Convergence criterion: Validation loss < 0.3 or no improvement for 30 epochs
\end{itemize}

This experimental setup ensures rigorous evaluation of GemGNN's architectural innovations while maintaining methodological integrity and enabling reliable comparison with existing approaches. The comprehensive parameter optimization and statistical analysis provide robust evidence for our framework's effectiveness in few-shot fake news detection.

% ------------------------------------------------
\EndChapter
% ------------------------------------------------