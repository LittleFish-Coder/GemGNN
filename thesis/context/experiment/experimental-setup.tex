% ------------------------------------------------
\StartChapter{Experimental Setup}{chapter:experimental-setup}
% ------------------------------------------------

This chapter presents the comprehensive experimental methodology designed to rigorously evaluate the GemGNN framework's core innovations in heterogeneous graph construction, multi-view learning, and few-shot fake news detection. Our experimental design emphasizes the authenticity of evaluation protocols and the logical validation of each architectural component's contribution to the overall system performance.

\section{Dataset Selection and Justification}

\subsection{FakeNewsNet Benchmark Datasets}

We conduct experiments on the FakeNewsNet benchmark \cite{shu2018fakenewsnet}, specifically utilizing the PolitiFact and GossipCop datasets. These datasets are selected not merely for their widespread adoption, but for their fundamental suitability to validate our approach's core hypotheses about content-based fake news detection in few-shot scenarios.

\subsubsection{PolitiFact: Political Misinformation Detection}

\textbf{Dataset Rationale:} Political news provides an ideal testbed for our content-based approach because political misinformation often contains subtle factual distortions embedded within otherwise accurate information. This characteristic allows us to evaluate whether our heterogeneous graph structure can capture nuanced semantic relationships that distinguish genuine from manipulated political content.

\textbf{Statistical Distribution:} 
\begin{itemize}
\item Training set: 246 real, 135 fake articles (381 total; 64.6\% real)
\item Test set: 73 real, 29 fake articles (102 total; 71.6\% real)  
\item Complete dataset: 319 real, 164 fake articles (483 total; 66.0\% real)
\end{itemize}

\textbf{Content Complexity:} Political articles in this dataset typically range from 200-800 words and contain factual assertions that can be independently verified. The class imbalance (2:1 real-to-fake ratio) reflects realistic deployment scenarios where legitimate news outnumbers fabricated content, making this dataset particularly suitable for evaluating few-shot performance under realistic conditions.

\subsubsection{GossipCop: Entertainment Content Validation}

\textbf{Dataset Rationale:} Entertainment news presents fundamentally different linguistic patterns and verification challenges compared to political content. Celebrity and entertainment articles often involve subjective interpretations, speculation, and sensational language, providing a complementary evaluation domain that tests our approach's generalization capabilities across content types.

\textbf{Statistical Distribution:}
\begin{itemize}
\item Training set: 7,955 real, 2,033 fake articles (9,988 total; 79.6\% real)  
\item Test set: 2,169 real, 503 fake articles (2,672 total; 81.2\% real)
\item Complete dataset: 10,124 real, 2,536 fake articles (12,660 total; 79.9\% real)
\end{itemize}

\textbf{Content Characteristics:} Entertainment articles typically exhibit more varied linguistic styles, emotional language, and speculative content compared to political news. The 4:1 real-to-fake ratio provides a different class balance that tests our framework's robustness to varying data distributions, while the larger dataset size (26x larger than PolitiFact) enables more comprehensive statistical analysis.

\subsection{Evaluation Protocol Authenticity}

\textbf{Content-Only Constraint:} Our experimental design explicitly focuses on content-based detection without relying on social propagation data, user behavior patterns, or network metadata. This constraint is not merely a limitation but a strategic design choice that ensures our approach remains applicable in scenarios where privacy regulations, platform restrictions, or real-time deployment requirements prevent access to social data.

\textbf{Professional Verification Standard:} Both datasets utilize professional fact-checker verification, providing high-confidence ground truth labels essential for reliable few-shot evaluation. The professional verification process ensures that our experimental results reflect genuine detection capability rather than biases in crowd-sourced or automated labeling.

\section{Core Architecture Components}

\subsection{DeBERTa Embedding Foundation}

\textbf{Architecture Selection Rationale:} We select DeBERTa (Decoding-enhanced BERT with Disentangled Attention) as our embedding foundation based on its unique architectural properties that enable effective multi-view learning. Unlike traditional transformers, DeBERTa's disentangled attention mechanism separates content and position representations, creating embeddings with superior partitioning characteristics essential for our multi-view approach.

\textbf{Embedding Generation Process:} Each news article undergoes processing through DeBERTa-base to generate 768-dimensional embeddings using the [CLS] token representation. This global document embedding captures comprehensive semantic information while maintaining the disentangled properties necessary for meaningful dimension partitioning in our multi-view construction.

\textbf{Multi-View Partitioning Strategy:} The 768-dimensional DeBERTa embeddings are systematically partitioned into multiple views (typically 3 views of 256 dimensions each), where each partition captures distinct semantic aspects of the content. This partitioning strategy leverages DeBERTa's internal attention structure to ensure that each view maintains discriminative power while focusing on different linguistic and semantic dimensions.

\subsection{Heterogeneous Graph Construction Pipeline}

\textbf{Dual Node Type Architecture:} Our heterogeneous graph employs two fundamental node types: (1) news nodes representing actual articles with DeBERTa embeddings, and (2) interaction nodes containing LLM-generated synthetic user responses. This dual-node design captures both content semantics and social interpretation patterns within a unified graph structure.

\textbf{Synthetic Interaction Generation:} For each news article, we generate 20 synthetic user interactions using large language models, distributed across three distinct tones: 8 neutral (factual focus), 7 affirmative (supportive), and 5 skeptical (questioning) interactions. This distribution reflects natural user response patterns while providing controlled variation in user perspective signals.

\textbf{Edge Construction Strategies:} We implement two complementary edge construction approaches:

\begin{itemize}
\item \textbf{Traditional KNN:} All nodes connect based on semantic similarity regardless of data partition, maximizing performance by leveraging full dataset connectivity. This approach provides upper-bound performance estimates and serves for deployment scenarios where articles can cross-reference.

\item \textbf{Test-Isolated KNN:} Test nodes connect only to other test nodes, while training nodes connect within their partition. This strategy prevents information leakage during evaluation, ensuring realistic performance assessment that reflects actual deployment conditions.
\end{itemize}

\textbf{Multi-View Edge Construction:} Within each edge construction strategy, we create multiple graph views by partitioning DeBERTa embeddings and computing separate similarity graphs for each partition. This multi-view approach captures diverse semantic perspectives that are aggregated through learned attention mechanisms in the heterogeneous graph neural network.

\subsection{Heterogeneous Graph Attention Network Architecture}

\textbf{Heterogeneous Attention Networks (HAN):} We employ HAN as our primary architecture due to its sophisticated handling of heterogeneous graph structures through hierarchical attention mechanisms. HAN operates at two levels: node-level attention for aggregating information from neighboring nodes of different types, and semantic-level attention for combining information across different edge types and meta-paths.

\textbf{Architecture Justification:} The selection of HAN is based on comprehensive empirical evaluation demonstrating superior performance compared to alternative heterogeneous graph neural network architectures. HAN's hierarchical attention mechanism proves particularly effective for fake news detection by enabling selective attention to relevant semantic relationships while maintaining computational efficiency in few-shot scenarios.

\textbf{Cross-Entropy Loss with Label Smoothing:} Based on comprehensive evaluation of multiple loss function variants, we employ cross-entropy loss with label smoothing as our training objective. This approach prevents overconfident predictions in few-shot scenarios through a smoothing factor of 0.1, providing optimal balance between learning signal strength and regularization effectiveness.

\section{Baseline Methods and Comparative Framework}

\subsection{Baseline Selection Strategy}

Our baseline selection follows a systematic approach to cover the full spectrum of fake news detection methodologies, enabling comprehensive evaluation of our approach's innovations across different paradigms.

\textbf{Traditional Content-Based Methods:}
\begin{itemize}
\item \textbf{Multi-Layer Perceptron (MLP):} Uses DeBERTa embeddings as static features for binary classification (hidden layers: 256, 128 units; ReLU activation; dropout: 0.3). Establishes performance baseline for content-only classification without structural information.

\item \textbf{Bidirectional LSTM:} Processes articles as word sequences with 128 hidden units. Tests whether sequential modeling provides advantages over static embeddings for fake news detection.
\end{itemize}

\textbf{Transformer-Based Language Models:}
\begin{itemize}
\item \textbf{BERT-base-uncased:} Fine-tuned for binary classification using [CLS] token representation (learning rate: 2e-5; batch size: 16; max length: 512 tokens).

\item \textbf{RoBERTa-base:} Optimized BERT variant with improved training procedures, using identical hyperparameters for fair comparison.
\end{itemize}

\textbf{Large Language Models:}
\begin{itemize}
\item \textbf{LLaMA-7B:} Evaluated through in-context learning with 2-3 examples per class from support set.

\item \textbf{Gemma-7B:} Complementary LLM evaluation using identical prompt engineering strategies.
\end{itemize}

\textbf{Graph-Based Methods:}
\begin{itemize}
\item \textbf{Less4FD:} Recent graph-based approach using KNN similarity graphs with GCN message passing.

\item \textbf{HeteroSGT:} Heterogeneous graph method adapted for content-only setting by removing social features.
\end{itemize}

\section{Few-Shot Evaluation Methodology}

\subsection{K-Shot Learning Protocol}

\textbf{Shot Configuration Rationale:} We evaluate across K ∈ {3, 4, 8, 16} shots per class, spanning from extremely few-shot (3-shot) to moderate few-shot (16-shot) scenarios. This range captures realistic deployment scenarios where labeled examples are scarce while providing sufficient statistical power for meaningful comparison.

\textbf{Support Set Sampling Strategy:} For each K-shot experiment, we employ stratified random sampling to select K examples per class from the training set. The sampling process ensures balanced representation across both classes and, where possible, different temporal periods and subtopics to minimize selection bias.

\textbf{Transductive Learning Framework:} Our evaluation employs transductive learning where all nodes (labeled training, unlabeled training, and test) participate in graph construction and message passing, but loss computation is restricted to labeled nodes. This paradigm maximizes the utility of available data while maintaining proper evaluation boundaries.

\textbf{Statistical Robustness:} We conduct 10 independent experimental runs for each configuration using different random seeds for support set sampling. Performance is reported as mean ± 95% confidence intervals across runs, ensuring reliable statistical inference despite the high variance inherent in few-shot learning.

\subsection{Performance Metrics and Statistical Analysis}

\textbf{Primary Metric Selection:} We employ F1-score as our primary evaluation metric due to the class imbalance present in both datasets (PolitiFact: 2:1 real-to-fake; GossipCop: 4:1 real-to-fake). F1-score provides a balanced assessment of precision and recall, making it particularly suitable for imbalanced few-shot scenarios where overall accuracy may be misleading.

\textbf{Comprehensive Metric Suite:} We report accuracy, precision, recall, and F1-score to provide complete performance characterization. This multi-metric approach reveals whether models exhibit class-specific biases and enables detailed analysis of failure modes.

\textbf{Statistical Significance Testing:} We employ paired t-tests to assess statistical significance of performance differences, accounting for the paired nature of few-shot experiments where identical support sets are used across methods. Bonferroni correction is applied for multiple comparisons across K-shot settings and datasets (α = 0.05).

\textbf{Effect Size Quantification:} Beyond statistical significance, we report Cohen's d effect sizes to quantify the practical significance of performance differences, ensuring that reported improvements represent meaningful advances rather than merely statistically detectable differences.

\section{Implementation Details and Experimental Configuration}

\subsection{Hyperparameter Selection and Optimization}

\textbf{Graph Construction Parameters:}
\begin{itemize}
\item K-nearest neighbors: k = 5 (optimized through grid search on {3, 5, 7, 10})
\item Multi-view partitioning: 3 views of 256 dimensions each from 768-dimensional DeBERTa embeddings
\item Synthetic interaction distribution: 20 interactions per article (8 neutral, 7 affirmative, 5 skeptical)
\item Similarity metric: Cosine similarity for all edge construction
\item Unlabeled sampling factor: 5x (unlabeled nodes = \texttt{num\_classes} $\times$ \texttt{k\_shot} $\times$ 5)
\end{itemize}

\textbf{Neural Network Architecture:}
\begin{itemize}
\item Hidden dimensions: 64 units in GNN layers (optimized from {32, 64, 128})
\item Attention heads: 4 heads for multi-head attention mechanisms
\item Network depth: 1 GNN layers (optimized from {1, 2, 3, 4})
\item Dropout rate: 0.3 for regularization (optimized from {0.1, 0.3, 0.5})
\item Activation function: ReLU throughout hidden layers
\end{itemize}

\textbf{Training Configuration:}
\begin{itemize}
\item Optimizer: Adam with learning rate 5e-4 (optimized from {1e-4, 5e-4, 1e-3})
\item Weight decay: 1e-3 for L2 regularization
\item Batch processing: Full graph training (transductive setting)
\item Maximum epochs: 300 with early stopping
\item Early stopping patience: 30 epochs
\item Convergence criterion: Validation loss < 0.3 or no improvement for 30 epochs
\end{itemize}

\subsection{Computational Infrastructure and Reproducibility}

\textbf{Hardware Configuration:} All experiments are conducted on NVIDIA A100 GPUs with 40GB memory, enabling efficient processing of large heterogeneous graphs and comprehensive hyperparameter exploration across 2,688 different parameter combinations.

\textbf{Software Environment:}
\begin{itemize}
\item Python 3.8 with PyTorch 1.12 for deep learning framework
\item PyTorch Geometric 2.1 for graph neural network implementations
\item Transformers library 4.20 for DeBERTa and baseline language models
\item CUDA 11.6 for GPU acceleration and optimization
\end{itemize}

\textbf{Reproducibility Measures:} We implement comprehensive reproducibility protocols including fixed random seeds for all stochastic processes (data sampling, model initialization, training), deterministic CUDA operations, and complete documentation of all hyperparameters, data splits, and experimental configurations.

\textbf{Performance Characteristics:} Training time ranges from 15-30 minutes per experimental run depending on dataset size and graph complexity. Memory requirements are approximately 8-12GB GPU memory for GossipCop (the larger dataset), well within modern research hardware capabilities. The efficient implementation enables comprehensive experimentation across multiple random seeds and parameter configurations.

This experimental setup ensures rigorous evaluation of GemGNN's architectural innovations while maintaining methodological integrity and enabling reliable comparison with existing approaches. The comprehensive parameter optimization and statistical analysis provide robust evidence for our framework's effectiveness in few-shot fake news detection.

% ------------------------------------------------
\EndChapter
% ------------------------------------------------