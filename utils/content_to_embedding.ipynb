{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content to Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littlefish/miniconda3/envs/fakenews/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Generating train split: 100%|██████████| 24353/24353 [00:00<00:00, 38969.44 examples/s]\n",
      "Generating validation split: 100%|██████████| 8117/8117 [00:00<00:00, 37654.04 examples/s]\n",
      "Generating test split: 100%|██████████| 8117/8117 [00:00<00:00, 32389.51 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# load and download the dataset from huggingface\n",
    "dataset = load_dataset(\"GonzaloA/fake_news\", download_mode=\"reuse_cache_if_exists\", cache_dir=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Type: <class 'datasets.dataset_dict.DatasetDict'>\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
      "        num_rows: 24353\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
      "        num_rows: 8117\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['Unnamed: 0', 'title', 'text', 'label'],\n",
      "        num_rows: 8117\n",
      "    })\n",
      "})\n",
      "Dataset keys: dict_keys(['train', 'validation', 'test'])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Type: {type(dataset)}\")\n",
    "print(f\"{dataset}\")\n",
    "print(f\"Dataset keys: {dataset.keys()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Validation dataset type: <class 'datasets.arrow_dataset.Dataset'>\n",
      "Test dataset type: <class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"]\n",
    "val_dataset = dataset[\"validation\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "print(f\"Train dataset type: {type(train_dataset)}\")\n",
    "print(f\"Validation dataset type: {type(val_dataset)}\")\n",
    "print(f\"Test dataset type: {type(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Unnamed: 0', 'title', 'text', 'label'])\n",
      "Title:  ‘Maury’ Show Official Facebook Posts F*CKED UP Caption On Guest That Looks Like Ted Cruz (IMAGE)\n",
      "Text: Maury is perhaps one of the trashiest shows on television today. It s right in line with the likes of the gutter trash that is Jerry Springer, and the fact that those shows are still on the air with the shit they air really is a sad testament to what Americans find to be entertaining. However, Maury really crossed the line with a Facebook post regarding one of their guest s appearance with a vile, disgusting caption on Tuesday evening.There was a young woman on there doing one of their episodes regarding the paternity of her child. However, on the page, the show posted an image of the woman, who happens to bear a striking resemblance to Senator and presidential candidate Ted Cruz. The caption from the Maury Show page read: The Lie Detector Test determined .that was a LIE!  Ted Cruz is just NOT that SEXY! As if that weren t horrible enough, the caption underneath the Imgur upload reads,  Ted Cruz in drag on Maury. Here is an image from the official Maury Facebook page:Here is the embed of the post itself:This is beyond despicable. It s bad enough that this show preys on desperate people to keep their trashy show going and their audience of bottom-feeders entertained, but now they publicly mock them as well? This young woman cannot help how she looks or who she resembles. That is not her fault. Shaming someone s looks on social media is something we d expect from the morons who watch this crap on a daily basis, but it is NOT something the official show page should be doing. Then again, what can you expect from a show that rolls in the mud for a living and continues to show the world that there is now low they will not stoop to? This was more than a step too far, though.Maury, you owe this young woman a public apology. A VERY public apology. There s just no excuse for this, no matter the demographics of your audience or what you do on that disgusting show of yours. I suppose it will be too much to ask that you lose viewers over this, because the people who watch your trashy ass show likely aren t educated enough to understand why this is so wrong in the first place. I don t watch, so I can t deprive you of my viewership, but I CAN call you out.Shame on you, Maury Show and everyone associated with this despicable Facebook post. You really showed your true colors here today.Featured image via Facebook \n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# First element of the train dataset\n",
    "print(f\"{train_dataset[0].keys()}\")\n",
    "print(f\"Title: {train_dataset[0]['title']}\")\n",
    "print(f\"Text: {train_dataset[0]['text']}\")\n",
    "print(f\"Label: {train_dataset[0]['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Tokenizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/littlefish/miniconda3/envs/fakenews/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert/distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(f\"distilbert/distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"distilbert/distilbert-base-uncased\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input keys: dict_keys(['input_ids', 'attention_mask'])\n",
      "Input ids: tensor([[  101,  5003, 13098,  2003,  3383,  2028,  1997,  1996, 11669, 10458,\n",
      "          3065,  2006,  2547,  2651,  1012,  2009,  1055,  2157,  1999,  2240,\n",
      "          2007,  1996,  7777,  1997,  1996,  9535,  3334, 11669,  2008,  2003,\n",
      "          6128, 17481,  1010,  1998,  1996,  2755,  2008,  2216,  3065,  2024,\n",
      "          2145,  2006,  1996,  2250,  2007,  1996,  4485,  2027,  2250,  2428,\n",
      "          2003,  1037,  6517,  9025,  2000,  2054,  4841,  2424,  2000,  2022,\n",
      "         14036,  1012,  2174,  1010,  5003, 13098,  2428,  4625,  1996,  2240,\n",
      "          2007,  1037,  9130,  2695,  4953,  2028,  1997,  2037,  4113,  1055,\n",
      "          3311,  2007,  1037, 25047,  1010, 19424, 14408,  3258,  2006,  9857,\n",
      "          3944,  1012,  2045,  2001,  1037,  2402,  2450,  2006,  2045,  2725,\n",
      "          2028,  1997,  2037,  4178,  4953,  1996,  6986, 11795,  3012,  1997,\n",
      "          2014,  2775,  1012,  2174,  1010,  2006,  1996,  3931,  1010,  1996,\n",
      "          2265,  6866,  2019,  3746,  1997,  1996,  2450,  1010,  2040,  6433,\n",
      "          2000,  4562,  1037,  8478, 14062,  2000,  5205,  1998,  4883,  4018,\n",
      "          6945,  8096,  1012,  1996, 14408,  3258,  2013,  1996,  5003, 13098,\n",
      "          2265,  3931,  3191,  1024,  1996,  4682, 19034,  3231,  4340,  1012,\n",
      "          2008,  2001,  1037,  4682,   999,  6945,  8096,  2003,  2074,  2025,\n",
      "          2008,  7916,   999,  2004,  2065,  2008,  4694,  1056,  9202,  2438,\n",
      "          1010,  1996, 14408,  3258,  7650,  1996, 10047, 27390,  2039, 11066,\n",
      "          9631,  1010,  6945,  8096,  1999,  8011,  2006,  5003, 13098,  1012,\n",
      "          2182,  2003,  2019,  3746,  2013,  1996,  2880,  5003, 13098,  9130,\n",
      "          3931,  1024,  2182,  2003,  1996,  7861,  8270,  1997,  1996,  2695,\n",
      "          2993,  1024,  2023,  2003,  3458,  4078, 24330,  3085,  1012,  2009,\n",
      "          1055,  2919,  2438,  2008,  2023,  2265,  8336,  2015,  2006,  7143,\n",
      "          2111,  2000,  2562,  2037, 11669,  2100,  2265,  2183,  1998,  2037,\n",
      "          4378,  1997,  3953,  1011, 21429,  2015, 21474,  1010,  2021,  2085,\n",
      "          2027,  7271, 12934,  2068,  2004,  2092,  1029,  2023,  2402,  2450,\n",
      "          3685,  2393,  2129,  2016,  3504,  2030,  2040,  2016, 12950,  1012,\n",
      "          2008,  2003,  2025,  2014,  6346,  1012, 25850,  2075,  2619,  1055,\n",
      "          3504,  2006,  2591,  2865,  2003,  2242,  2057,  1040,  5987,  2013,\n",
      "          1996, 22822,  5644,  2040,  3422,  2023, 10231,  2006,  1037,  3679,\n",
      "          3978,  1010,  2021,  2009,  2003,  2025,  2242,  1996,  2880,  2265,\n",
      "          3931,  2323,  2022,  2725,  1012,  2059,  2153,  1010,  2054,  2064,\n",
      "          2017,  5987,  2013,  1037,  2265,  2008,  9372,  1999,  1996,  8494,\n",
      "          2005,  1037,  2542,  1998,  4247,  2000,  2265,  1996,  2088,  2008,\n",
      "          2045,  2003,  2085,  2659,  2027,  2097,  2025,  2358, 18589,  2000,\n",
      "          1029,  2023,  2001,  2062,  2084,  1037,  3357,  2205,  2521,  1010,\n",
      "          2295,  1012,  5003, 13098,  1010,  2017, 12533,  2023,  2402,  2450,\n",
      "          1037,  2270, 12480,  1012,  1037,  2200,  2270, 12480,  1012,  2045,\n",
      "          1055,  2074,  2053,  8016,  2005,  2023,  1010,  2053,  3043,  1996,\n",
      "         28321,  1997,  2115,  4378,  2030,  2054,  2017,  2079,  2006,  2008,\n",
      "         19424,  2265,  1997,  6737,  1012,  1045,  6814,  2009,  2097,  2022,\n",
      "          2205,  2172,  2000,  3198,  2008,  2017,  4558,  7193,  2058,  2023,\n",
      "          1010,  2138,  1996,  2111,  2040,  3422,  2115, 11669,  2100,  4632,\n",
      "          2265,  3497,  4995,  1056,  5161,  2438,  2000,  3305,  2339,  2023,\n",
      "          2003,  2061,  3308,  1999,  1996,  2034,  2173,  1012,  1045,  2123,\n",
      "          1056,  3422,  1010,  2061,  1045,  2064,  1056,  2139, 18098,  3512,\n",
      "          2017,  1997,  2026,  7193,  5605,  1010,  2021,  1045,  2064,  2655,\n",
      "          2017,  2041,  1012,  9467,  2006,  2017,  1010,  5003, 13098,  2265,\n",
      "          1998,  3071,  3378,  2007,  2023,  4078, 24330,  3085,  9130,  2695,\n",
      "          1012,  2017,  2428,  3662,  2115,  2995,  6087,  2182,  2651,  1012,\n",
      "          2956,   102]], device='cuda:0')\n",
      "Attention mask: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# get the input ids\n",
    "inputs = tokenizer(train_dataset[0]['text'], return_tensors=\"pt\", truncation=True).to(device)\n",
    "print(f\"Input keys: {inputs.keys()}\")\n",
    "print(f\"Input ids: {inputs['input_ids']}\")\n",
    "print(f\"Attention mask: {inputs['attention_mask']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the embeddings of the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.output_hidden_states = True\n",
    "\n",
    "# Get model output with hidden states\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Now, outputs will have the hidden states\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "# The last layer's hidden state can be accessed like this\n",
    "last_hidden_state = hidden_states[-1]\n",
    "\n",
    "# If you still want to extract embeddings similar to the previous approach\n",
    "embeddings = last_hidden_state.mean(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 hidden states\n",
      "Shape of the last hidden state: torch.Size([1, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(hidden_states)} hidden states\")\n",
    "print(f\"Shape of the last hidden state: {last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings shape: torch.Size([1, 768])\n",
      "Embeddings: tensor([[-3.1861e-02,  4.9893e-02,  2.0683e-01,  2.3420e-02,  3.4852e-02,\n",
      "         -8.9138e-02,  1.2603e-01,  6.1797e-01, -1.6187e-02, -8.3815e-02,\n",
      "          7.8477e-02, -3.5581e-01, -2.5342e-01,  3.8328e-01, -2.8318e-01,\n",
      "          4.2140e-01,  2.3357e-01,  1.2756e-01, -1.7217e-01,  3.9825e-01,\n",
      "          2.3975e-01, -1.0083e-01,  3.2119e-02,  3.6854e-01,  2.3199e-01,\n",
      "         -1.9419e-02,  6.1420e-02, -1.5400e-01, -1.5163e-01, -4.9244e-02,\n",
      "          5.3825e-01, -1.7541e-01, -8.5408e-02, -1.3305e-01, -8.4796e-02,\n",
      "         -2.1256e-01,  6.0817e-02, -8.8527e-02, -2.5797e-02,  2.2620e-01,\n",
      "         -6.4063e-01, -2.0540e-01, -2.6151e-02,  8.0417e-03, -3.1432e-01,\n",
      "         -2.7633e-01,  3.7468e-01,  4.8099e-02,  8.5311e-02,  8.4348e-03,\n",
      "         -2.2293e-01,  4.1168e-01, -1.1108e-01,  5.8610e-02,  2.3323e-01,\n",
      "          3.8766e-01, -1.2526e-01, -3.7300e-01, -5.1305e-01, -2.0898e-01,\n",
      "          1.3066e-01,  8.2867e-02, -3.6408e-02, -5.3573e-01,  4.9963e-02,\n",
      "          2.4954e-01, -3.9093e-03,  3.9631e-01, -6.2606e-01, -2.1422e-01,\n",
      "         -3.1353e-01, -3.3213e-01, -1.8269e-02, -8.7372e-03,  4.0488e-02,\n",
      "         -7.9459e-02, -2.8884e-02,  3.1328e-01,  3.2957e-02, -2.8761e-02,\n",
      "         -1.4315e-01,  3.6034e-01, -3.0775e-01,  4.9397e-01, -2.0651e-02,\n",
      "         -2.0548e-02,  1.3172e-01,  2.0254e-01, -2.9816e-01,  3.3343e-01,\n",
      "         -4.0098e-02, -2.1147e-01,  2.8911e-01, -9.1051e-03,  1.2517e-01,\n",
      "         -2.3937e-01,  3.3775e-01,  1.8701e-02, -2.3323e-01,  3.0860e-01,\n",
      "          1.7527e-02, -4.0524e-01,  2.5425e-01, -2.7025e-03,  6.4249e-02,\n",
      "         -1.7715e-01,  3.6724e-01,  2.4097e-01, -1.7417e-01,  3.9565e-01,\n",
      "          1.4051e-01, -2.8895e-02, -9.4666e-02, -2.8945e-01, -1.5096e-01,\n",
      "          1.2924e-01,  4.4954e-03, -2.6395e-01,  3.0386e-02,  1.3939e-01,\n",
      "          1.8303e-01, -3.0104e-01,  7.1675e-02,  4.4658e-01, -3.8962e-02,\n",
      "          1.0156e-01, -9.6257e-02,  3.6385e-01, -1.8011e-01,  3.3034e-02,\n",
      "          3.3123e-01,  2.8440e-01,  2.1129e-01, -1.8174e-01, -2.3620e-01,\n",
      "          1.7466e-01, -1.1271e-02, -3.9259e-01, -2.6664e-01,  1.2266e-01,\n",
      "          4.6157e-02, -1.5361e-01,  9.0505e-02,  2.4043e-01,  3.2349e-02,\n",
      "         -1.3750e-01, -8.3587e-02, -1.4703e-01,  2.3299e-02,  5.4441e-03,\n",
      "          4.9412e-02, -8.1886e-02, -2.8583e-02, -2.9443e-01, -5.5387e-02,\n",
      "         -1.9958e-01, -2.7795e-01, -2.2185e-02, -5.0495e-03,  1.4318e-01,\n",
      "          4.7146e-01, -1.5953e-01, -1.6882e-01,  1.7309e-01,  1.6876e-01,\n",
      "          1.0646e-01,  1.5797e-02,  3.6860e-01, -1.1341e-01,  1.0786e-01,\n",
      "         -9.2945e-02, -9.8868e-02,  7.8469e-01,  6.0573e-02,  1.6102e-01,\n",
      "          1.1333e-01,  2.4260e-01,  4.4692e-02,  2.7324e-01,  9.0933e-02,\n",
      "         -6.9970e-01,  3.1096e-01,  4.8355e-02,  7.6853e-02,  2.1501e-01,\n",
      "         -2.2787e-01,  2.7263e-01, -2.0819e-01,  3.9397e-02, -1.1637e-01,\n",
      "         -4.2994e-01, -1.9461e-01, -2.3559e-01,  1.3564e-02,  2.0738e-01,\n",
      "         -2.8563e-01, -2.1637e-01,  6.5830e-02, -2.2249e-01, -6.0595e-02,\n",
      "         -1.3586e-01,  6.3180e-02,  2.0413e-01,  1.4150e-01, -2.5984e-01,\n",
      "         -2.3837e-01,  4.2281e-02, -2.9588e-01, -2.6826e-02,  2.2743e-01,\n",
      "         -1.1206e-01,  2.4855e-01,  2.3458e-02,  1.0281e-01, -2.1229e-01,\n",
      "          1.4560e-01,  1.2761e-01, -2.5393e-01,  5.4477e-02, -3.7378e-02,\n",
      "         -1.0965e-01,  8.7899e-02, -4.0052e-01,  2.0377e-01, -1.0696e-01,\n",
      "          6.6292e-01, -5.2410e-02, -5.6441e-01,  2.5559e-01,  4.1176e-01,\n",
      "          6.0174e-03, -3.4034e-02,  5.1234e-01, -1.4643e-01, -1.7498e-01,\n",
      "         -9.5714e-02, -1.8252e-01, -1.7573e-01,  6.9891e-02, -3.1731e-01,\n",
      "         -2.5956e-01,  5.1388e-01,  2.0889e-01,  1.3582e-01, -1.0635e-01,\n",
      "         -9.7069e-02, -1.1633e-01,  3.6164e-02, -2.0558e-01, -1.8921e-01,\n",
      "         -4.1887e-01, -2.8280e-01, -7.8193e-02, -4.4247e-01, -2.5848e-02,\n",
      "         -9.8354e-02, -2.6611e-01, -1.4883e-01,  4.4767e-02,  2.2310e-02,\n",
      "          2.2095e-01, -1.7768e-02, -9.1550e-02,  1.2168e-01, -4.4045e-01,\n",
      "         -2.9103e-01,  6.5907e-02,  3.2937e-01,  4.6146e-02,  2.9692e-01,\n",
      "          1.8695e-01,  8.6950e-02, -6.5272e-02,  6.5201e-01, -1.2698e-01,\n",
      "          1.0687e-02,  1.9869e-01,  1.8375e-01, -2.5135e-02, -1.5605e-01,\n",
      "          8.5086e-02,  3.0088e-01, -1.5563e-01,  1.0152e-01, -1.1133e-02,\n",
      "         -3.5511e-01,  3.3646e-01,  8.6695e-02, -2.5921e-01, -4.0216e-02,\n",
      "         -1.0202e-01,  1.7810e-01, -1.9350e-01, -2.0241e-01,  2.4253e-01,\n",
      "         -8.8621e-02,  3.9106e-01,  6.4107e-02,  9.2825e-02, -9.2102e-02,\n",
      "         -1.1040e-01, -2.1611e-01,  3.6839e-01,  9.6007e-02,  4.5294e-03,\n",
      "          2.2980e-01,  7.9627e-02, -4.3978e-01, -3.9782e+00,  7.2683e-02,\n",
      "          4.4119e-01, -2.5857e-01,  2.3716e-01, -2.6316e-01, -3.1856e-03,\n",
      "         -1.8667e-01, -3.9367e-01,  2.8070e-01,  5.1201e-02, -2.9684e-01,\n",
      "          2.0174e-01,  2.4882e-01,  2.9507e-01, -3.0748e-01,  8.6577e-02,\n",
      "         -2.1215e-01, -1.2137e-01,  3.7853e-01, -2.1518e-01, -3.5641e-01,\n",
      "          1.0450e-01, -1.4838e-01,  3.7539e-01,  7.5300e-01, -2.2837e-01,\n",
      "         -1.8895e-01, -4.3926e-02, -1.0467e-01,  6.1091e-02, -2.3738e-01,\n",
      "          2.7883e-02,  2.2131e-01,  6.3001e-02, -5.3498e-02, -9.6353e-03,\n",
      "         -4.7614e-01, -2.0977e-01, -2.9186e-01, -1.0274e-01, -6.5303e-01,\n",
      "         -5.0652e-02, -4.4882e-03,  6.6350e-01, -1.1178e-01, -4.6172e-02,\n",
      "         -3.6923e-01,  2.4081e-01,  1.6479e-02,  2.2184e-01,  1.5827e-01,\n",
      "         -2.3527e-01,  1.3755e-01,  5.9683e-03, -6.4751e-02,  3.9467e-01,\n",
      "          3.5535e-01, -1.8324e-01, -4.0171e-01, -5.7211e-02, -7.7770e-02,\n",
      "         -4.9304e-01,  1.3639e-01, -6.2594e-02, -4.4269e-01, -2.3803e-01,\n",
      "         -1.6764e-02,  1.6177e-01, -4.5843e-02, -1.2000e-01,  3.6500e-01,\n",
      "         -2.7641e-01, -1.2025e-01,  2.4294e-02, -2.3793e-01,  8.9434e-02,\n",
      "         -1.7573e-01, -4.8661e-02, -1.0609e-01, -1.0650e-01, -3.1113e-01,\n",
      "         -5.6472e-02, -1.2472e-01, -2.0601e-01, -5.3020e-01,  1.5546e-01,\n",
      "         -3.5363e-02, -2.9255e-01, -4.4037e-01,  1.8744e-01,  1.9367e-01,\n",
      "          1.1330e-02,  2.1545e-01,  1.2170e-01,  4.0542e-02,  3.2108e-01,\n",
      "         -2.0925e-02,  2.0740e-02, -2.1470e-01,  1.7355e-02, -1.2994e-01,\n",
      "          5.2343e-01, -2.2427e-01,  4.4996e-02, -5.4961e-02, -2.4166e-01,\n",
      "          1.4071e-01, -6.0373e-02, -7.6321e-02,  2.2183e-01, -2.4053e-01,\n",
      "          2.7053e-01, -1.6504e-01,  1.5252e-02, -1.0709e-01,  1.7263e-01,\n",
      "          3.7897e-01, -1.6705e-01, -1.1007e-01,  1.5139e-02,  5.1862e-01,\n",
      "         -4.4150e-01, -2.4136e-01, -3.8033e-01, -3.2364e-02, -1.1220e-01,\n",
      "          1.5758e-01,  1.3544e-02,  5.7771e-02, -1.5276e-01, -7.9758e-03,\n",
      "         -1.4398e-01,  2.7136e-01,  2.5783e-01,  2.4278e-02,  8.3501e-02,\n",
      "         -5.7210e-01, -8.5838e-03,  9.7191e-02,  1.2175e-01,  1.9393e-01,\n",
      "         -1.5527e-01, -3.7067e-02, -1.4457e-01,  4.3437e-01,  5.4536e-02,\n",
      "          3.5632e-02, -1.2356e-01,  6.3486e-02, -5.3974e-01, -1.6422e-01,\n",
      "         -1.7104e-01, -1.8566e-01,  3.1535e-01,  3.3788e-01, -3.5389e-02,\n",
      "         -2.4914e-02, -1.9903e-02, -4.1820e-01,  6.1305e-02, -4.3885e-02,\n",
      "          1.5456e-01,  9.8436e-02, -1.9965e-01,  2.6820e-01, -2.9748e-02,\n",
      "         -1.2027e-01, -4.6402e-02,  3.5516e-02, -1.5227e-01,  3.9478e-02,\n",
      "          6.6159e-02, -1.3919e-01, -1.1751e-01,  3.7992e-01, -3.8060e-02,\n",
      "         -9.3320e-02, -9.5741e-02,  3.7420e-01, -1.4205e-01, -1.0972e-01,\n",
      "         -5.3370e-02, -1.4929e-01,  3.5027e-01,  1.2348e-01,  2.4999e-01,\n",
      "         -1.2900e-01, -1.3606e-01,  4.7005e-02, -1.8440e-01,  3.3623e-01,\n",
      "         -8.7114e-03, -5.9844e-02, -4.3982e-01, -2.0538e-01,  3.1099e-01,\n",
      "         -4.5494e-01,  6.8332e-02,  1.2561e-01,  9.6228e-02,  3.3237e-02,\n",
      "         -4.6838e-01,  2.0250e-01,  6.2327e-02, -2.6245e-01,  5.1629e-02,\n",
      "          3.1942e-01,  6.4149e-02,  1.7281e-01, -4.3232e-01, -2.2310e-01,\n",
      "         -4.0110e-01, -3.9921e-02,  1.6741e-01, -5.1360e-01, -4.7912e-02,\n",
      "         -2.1086e-01, -1.8692e-01,  6.4303e-02, -1.7037e-01, -1.6931e-01,\n",
      "         -7.1401e-02, -3.7308e-02, -4.0681e-01, -1.0378e-01,  1.8559e-03,\n",
      "         -5.5730e-02, -1.0916e-01, -3.2308e-01,  2.7269e-01, -7.0883e-01,\n",
      "          1.8537e-01,  2.9434e-01, -3.1048e-01, -5.0429e-03,  1.2759e-02,\n",
      "         -3.5826e-01, -7.9308e-02,  3.8953e-02,  2.7626e-02,  3.8860e-02,\n",
      "         -1.8076e-01, -2.4477e-01,  3.7743e-01, -8.2583e-02, -1.0616e-01,\n",
      "          3.5702e-01, -1.6958e-01,  2.8995e-01, -9.2242e-02, -2.4080e-02,\n",
      "         -1.3308e-01, -2.0149e-01, -3.0472e-02, -6.9292e-01, -3.9283e-01,\n",
      "          1.2190e-01, -1.8215e-01, -1.1664e-01, -1.5799e-01, -8.6644e-02,\n",
      "         -3.6048e-02,  1.2333e-01,  1.5664e-01, -4.7211e-02,  1.2128e-01,\n",
      "          3.0378e-01, -2.0309e-03, -2.7077e-01,  1.5238e-01, -6.3089e-02,\n",
      "         -1.1788e-01, -1.2281e-01,  2.5374e-02,  2.3254e-01, -1.0653e-01,\n",
      "         -4.3129e-01,  2.0842e-01, -4.4149e-01, -1.0443e-01, -2.5931e-02,\n",
      "         -9.4239e-02, -2.1314e-01,  2.5397e-01, -4.9658e-01,  2.1717e-01,\n",
      "          4.4538e-01, -1.6365e-01, -1.2070e-01,  2.0918e-01,  1.8336e-01,\n",
      "          4.6265e-02, -1.4200e-02, -3.3528e-02, -6.3738e-02,  5.9025e-01,\n",
      "          1.9481e-01,  8.9471e-02, -1.0735e-01, -6.5447e-02, -2.2690e-01,\n",
      "          1.8863e-01,  2.6298e-01, -2.2197e-01, -4.6688e-02,  8.1292e-02,\n",
      "         -4.3673e-01,  3.6142e-02,  2.2848e-01, -2.3737e-01, -3.0275e-01,\n",
      "          6.2966e-01,  2.2233e-01, -2.7849e-01, -1.2992e-01, -2.5377e-03,\n",
      "         -9.3918e-03, -6.5833e-02, -9.7761e-02,  1.4268e-01, -1.3296e-01,\n",
      "          4.7377e-01, -4.3782e-02,  3.2143e-01,  1.5153e-01, -3.1171e-01,\n",
      "         -1.3485e-01,  1.4189e-01,  2.0741e-01,  1.8391e-03,  5.0331e-01,\n",
      "         -3.2475e-01,  3.5097e-01, -1.4263e-01, -2.7001e-01,  4.0036e-04,\n",
      "          9.5695e-02,  1.6871e-01,  2.1560e-02, -1.1070e-01,  4.0908e-02,\n",
      "          4.7688e-01,  4.9483e-01,  3.3918e-01,  2.4443e-01,  2.2473e-01,\n",
      "         -1.0086e-01,  6.1391e-01,  3.6069e-01,  4.2420e-02,  2.7688e-01,\n",
      "         -6.1879e-02, -6.6045e-02,  4.2411e-01,  1.9089e-01,  3.1645e-01,\n",
      "          2.0144e-01, -1.3929e-01,  5.8144e-01,  1.9958e-01,  2.2804e-01,\n",
      "          2.1350e-01, -2.4196e-01, -9.9079e-02,  1.8281e-01,  9.3320e-02,\n",
      "         -2.6377e-01, -5.3543e-02,  5.0373e-02,  1.8315e-01,  1.3552e-01,\n",
      "         -8.7507e-02, -1.6146e-01, -5.8902e-02,  5.0296e-01,  1.1303e-01,\n",
      "         -1.5579e-01, -2.1369e-01,  1.0159e-01,  5.2980e-03,  8.5992e-02,\n",
      "         -2.6480e-01, -1.0031e-01, -8.9724e-02, -1.3379e-01, -1.2096e-01,\n",
      "         -1.0062e-01, -5.8135e-02, -4.0331e-01, -3.9100e-02, -1.1505e-01,\n",
      "          9.9159e-02, -1.0182e-01, -1.7224e-01, -1.7323e-01,  3.1937e-01,\n",
      "          1.3625e-01,  3.9891e-01,  4.4036e-02,  1.2825e-01, -1.1148e-01,\n",
      "         -1.5290e-01,  1.8389e-01,  3.5309e-02,  5.2081e-02,  5.0051e-02,\n",
      "          1.0304e-01, -4.3439e-01, -4.7513e-02,  8.4965e-02,  3.1779e-01,\n",
      "         -4.0173e-01, -5.4758e-02, -1.2681e-01,  7.8614e-02,  1.4218e-01,\n",
      "          5.4765e-02, -1.8840e-01, -4.6900e-02, -2.6009e-01, -3.0046e-01,\n",
      "         -1.0876e-01,  2.7011e-01, -1.0662e-01, -6.3535e-03,  2.5936e-01,\n",
      "          1.5752e-01, -9.6157e-02, -1.8164e-01, -1.8608e-01,  1.8352e-02,\n",
      "         -7.2576e-02,  3.1163e-01,  6.4218e-03, -3.1919e-01, -8.5410e-02,\n",
      "          2.0877e-01, -1.7275e-01,  1.9664e-02,  1.8191e-01,  9.4675e-02,\n",
      "         -7.9734e-02, -1.6121e-01,  5.6266e-01, -2.6741e-02, -3.8845e-02,\n",
      "         -2.3356e-01,  6.1577e-02, -1.4826e-01, -1.0518e-01,  1.7496e-02,\n",
      "          4.9037e-02, -2.8265e-01, -1.5030e-01, -2.2750e-01,  9.6979e-02,\n",
      "         -1.4032e-02,  2.6561e-01,  3.4179e-02]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"Embeddings: {embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Class Model\n",
    "\n",
    "Now that we know how to get the embeddings of the content, \n",
    "\n",
    "we can create a class model that will take the dataset and get the embeddings of the content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'datasets.arrow_dataset.Dataset'>\n"
     ]
    }
   ],
   "source": [
    "print(type(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, model_name='bert-base-uncased', max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name, clean_up_tokenization_spaces=True)\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        # Tokenize the text\n",
    "        inputs = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')\n",
    "\n",
    "        # Move input tensors to the right shape for model (batch_size=1)\n",
    "        input_ids = inputs['input_ids'].squeeze(0)  # Shape: (seq_len,)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)  # Shape: (seq_len,)\n",
    "\n",
    "        # Get the BERT embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(input_ids.unsqueeze(0), attention_mask=attention_mask.unsqueeze(0))\n",
    "            embeddings = outputs.last_hidden_state.squeeze(0)  # Shape: (seq_len, hidden_dim)\n",
    "\n",
    "        # Pool the embeddings (here, we use the [CLS] token embedding)\n",
    "        cls_embedding = embeddings[0]  # Shape: (hidden_dim,)\n",
    "\n",
    "        return cls_embedding, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
      "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "new_dataset = CustomDataset(texts=train_dataset['text'], labels=train_dataset['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset length: 24353\n",
      "Dataset first element: (tensor([ 3.4876e-02, -5.6218e-01,  2.0681e-01, -5.0859e-02, -6.7468e-02,\n",
      "        -8.2454e-01,  4.2402e-01,  5.0450e-01,  6.3672e-02, -2.4987e-01,\n",
      "         2.3391e-01,  3.1432e-02, -2.4666e-01,  5.9632e-02,  4.0116e-01,\n",
      "         3.6769e-01, -5.6225e-02,  6.0175e-01,  5.6815e-01,  2.3334e-01,\n",
      "        -4.0697e-01, -5.5781e-01,  7.2422e-01, -5.2590e-01, -1.6810e-02,\n",
      "        -2.6917e-01, -1.9714e-01, -2.8483e-01, -5.2153e-01, -4.6544e-02,\n",
      "        -2.3110e-01,  6.0273e-02, -3.3898e-01, -3.2351e-01,  4.3331e-01,\n",
      "        -4.3363e-01,  1.3181e-01,  9.5156e-02,  2.9911e-01,  2.3036e-01,\n",
      "        -2.7992e-01,  4.2920e-01, -7.2668e-02,  2.4537e-02, -5.7243e-02,\n",
      "        -2.2982e-01, -4.0438e+00,  4.4943e-01,  1.1087e-01, -2.8877e-01,\n",
      "         5.0105e-01, -3.7387e-01, -2.3249e-01,  4.1634e-01,  4.5004e-01,\n",
      "         4.4103e-01, -8.9959e-01, -8.5779e-02,  1.3592e-01, -1.5080e-01,\n",
      "         1.4691e-01, -2.4806e-01, -1.0559e-01, -5.5534e-01, -1.4635e-01,\n",
      "         2.5595e-01,  1.4720e-02,  4.0702e-01, -6.6065e-01,  3.6912e-01,\n",
      "        -3.2182e-01, -3.1020e-01,  4.2643e-01, -3.1291e-01,  5.7939e-01,\n",
      "         8.5819e-02, -1.8616e-01,  5.2965e-01, -6.7565e-01, -1.4754e-01,\n",
      "        -9.3748e-02,  3.0538e-01,  5.5916e-02, -3.4849e-02,  1.3589e-01,\n",
      "         2.9479e-01,  2.5804e-02,  4.1447e-02,  3.6150e-01,  2.5587e-01,\n",
      "        -2.0147e-01,  5.0154e-01, -9.4489e-02,  1.3542e-01,  5.7874e-01,\n",
      "        -4.1780e-01,  3.6487e-01,  1.9466e-03, -5.2009e-02,  4.4320e-01,\n",
      "        -1.3777e-02,  4.2148e-02,  7.4484e-02, -9.4684e-01,  3.5150e-02,\n",
      "        -1.8159e-02,  3.1282e-01, -6.4838e-01,  1.2241e-01, -1.6500e+00,\n",
      "         2.1710e-01,  1.1038e+00,  7.6118e-02, -7.7650e-01, -2.9056e-01,\n",
      "         4.5828e-01,  5.7321e-01, -8.1299e-01,  3.0548e-01,  1.5352e-01,\n",
      "         1.3164e-01,  1.5059e-01, -1.1599e-01, -4.3344e-01,  9.3050e-02,\n",
      "         8.4943e-02,  3.6368e-01, -3.9101e-01,  4.8812e-02,  3.6622e-01,\n",
      "         4.3292e-01,  3.0904e-01, -3.5078e-02, -1.5215e-02, -3.8326e-01,\n",
      "         2.8878e-01,  1.6210e-01, -4.3024e-01, -3.8302e-01, -2.5724e-01,\n",
      "        -5.6686e-01, -5.5219e-01, -2.5892e+00,  2.6275e-01,  4.8261e-01,\n",
      "         2.9119e-01,  4.1958e-02,  6.5084e-01, -3.0255e-01, -2.2077e-01,\n",
      "         7.1716e-02, -2.4775e-01, -1.7229e-01, -9.8279e-02, -7.3859e-02,\n",
      "         1.3350e-02, -2.5725e-01, -3.9427e-01,  2.6827e-01,  8.6547e-01,\n",
      "         6.5148e-01,  4.0016e-01, -1.0179e-01, -1.5429e-01,  1.9280e-02,\n",
      "         2.5650e-01, -3.8762e-02,  4.6889e-01, -3.7575e-02, -4.3225e-01,\n",
      "        -2.9170e-02,  6.2477e-01,  5.7200e-01, -2.5431e-02,  1.7541e-01,\n",
      "         2.7605e-01,  3.0255e-01,  2.2304e-01,  4.9101e-01, -1.9055e-01,\n",
      "        -9.2536e-01,  2.5021e-01, -3.3002e-02,  1.2502e-01,  2.8595e-01,\n",
      "        -1.3432e-01,  6.8636e-01, -5.9917e-02, -1.0299e-01,  4.8007e-02,\n",
      "        -6.3117e-01, -3.9975e-01,  4.5679e-01,  5.2305e-01,  9.8591e-01,\n",
      "         4.7110e-02,  2.1880e-01,  6.4980e-02,  9.4796e-02,  2.1855e-01,\n",
      "        -2.0146e-01,  1.9059e-02,  3.0504e-01, -2.0279e-01, -7.1462e-01,\n",
      "         4.2621e+00,  7.3075e-02, -1.1519e-01, -9.2338e-02,  2.3201e-01,\n",
      "        -1.7133e-01, -4.7292e-01, -3.3236e-01,  2.0607e-01, -1.8550e-01,\n",
      "         2.7687e-01,  5.6722e-01,  2.7958e-02, -3.1801e-01, -7.2970e-02,\n",
      "         2.5107e-01,  3.3775e-01, -4.3412e-01,  5.9669e-01, -4.9433e-01,\n",
      "         1.2427e-01, -3.2426e-01,  3.4743e-01,  7.1149e-01, -1.6913e+00,\n",
      "        -8.8273e-02,  1.5112e-03, -7.7020e-01,  3.7741e-02, -1.5351e-01,\n",
      "         3.5046e-02, -1.0732e-01, -4.5916e-01, -3.4684e-01, -5.0859e-02,\n",
      "         4.2852e-01,  8.2990e-01,  4.5697e-02,  3.6080e-01, -6.8436e-01,\n",
      "         1.7476e-01, -2.4376e-01,  6.7934e-02,  4.2866e-01, -9.2695e-02,\n",
      "         4.4003e-01,  3.6138e-01,  6.5550e-01,  4.0007e-01,  6.1323e-01,\n",
      "         5.9200e-01, -3.5355e-01,  1.3607e-01, -5.1569e-01, -4.5622e-01,\n",
      "        -2.7350e-01,  7.5364e-02, -1.2070e-01,  2.2561e-01, -6.9649e-01,\n",
      "        -1.4579e-01, -6.5126e-02, -5.5415e-01, -2.9564e-02, -1.1596e-01,\n",
      "         2.5980e-01, -1.4336e-01, -1.1383e+00, -2.3399e+00,  2.5907e-01,\n",
      "         1.1466e-01,  4.5527e-01,  2.0001e-01, -1.5435e-01, -1.5622e-01,\n",
      "         5.4568e-02,  4.5017e-01, -2.5762e-01,  5.9838e-01,  4.2745e-01,\n",
      "        -8.1024e-01,  2.2465e-01, -2.2035e-01, -3.4081e-02,  6.5293e-01,\n",
      "        -1.0045e-01, -5.0331e-01,  3.4554e-02, -1.8935e-01,  3.8235e-01,\n",
      "        -6.9138e-01,  6.4005e-01, -1.2632e-01,  1.7563e-01,  5.0179e-03,\n",
      "        -9.7240e-01,  9.7147e-02, -2.0703e-01,  5.7073e-01, -1.1890e-01,\n",
      "         2.0966e-01, -2.3769e-01, -8.3262e-01, -3.1301e+00,  2.0800e-01,\n",
      "         2.1748e-02, -2.0288e-01, -2.9075e-01, -4.6603e-02,  3.2014e-01,\n",
      "        -2.7045e-01, -5.0890e-01,  6.8669e-02,  2.9242e-01,  2.7772e-02,\n",
      "         2.0280e-01, -2.6795e-01,  8.1056e-01, -4.7901e-03,  3.0920e-01,\n",
      "        -3.4428e-01,  4.6832e-01,  1.2947e-01, -5.1782e-01, -2.9853e-01,\n",
      "         3.5317e-01, -2.8882e-01,  4.8979e-01,  8.8034e-01, -9.2431e-01,\n",
      "         4.6041e-01, -8.6261e-03,  2.1064e-01,  6.7886e-01, -2.8554e-01,\n",
      "        -4.2377e-01, -1.6627e-02, -2.6803e-01, -3.6874e-01,  1.1938e-01,\n",
      "        -3.6757e-01,  4.3164e-01, -3.1761e-01,  6.8197e-01,  5.9433e-01,\n",
      "         2.7694e-02, -2.8078e-02,  1.0522e+00, -1.4015e-02,  1.9633e-01,\n",
      "        -2.6446e-01,  3.0880e-04,  3.0112e-01,  1.7806e-01,  2.7345e-01,\n",
      "         1.1477e+00,  1.9140e-01,  7.4804e-02,  6.7713e-02,  2.5063e-01,\n",
      "         5.5826e-02, -3.2179e-02,  4.4927e-01,  1.0511e+00, -4.4737e-01,\n",
      "         2.9215e-01, -8.6233e-02,  3.2910e-01, -5.7328e-01,  1.9159e-01,\n",
      "        -5.1695e-01,  1.3205e-01, -3.4732e-02, -1.9865e-02,  6.4675e-01,\n",
      "        -2.1136e-01, -1.3615e+00,  3.6112e-02, -3.4928e-01, -3.7550e-01,\n",
      "         1.7401e-01,  1.0078e-01,  3.1275e-01, -8.5118e-02, -1.9843e-01,\n",
      "        -3.9980e-01,  1.5793e-01, -3.9223e-01, -4.8797e-01,  3.8826e-01,\n",
      "         2.7657e-01, -5.8315e-01,  1.7478e-02, -1.8193e-02,  1.7611e-01,\n",
      "         2.9019e-01,  7.7282e-01, -4.2524e-01,  9.9005e-02,  5.3233e-01,\n",
      "        -8.0368e-01,  3.8987e-01,  2.8746e-01,  5.0320e-01,  1.9232e-01,\n",
      "         3.5376e-01, -2.5713e-01, -2.4027e-01,  2.7354e-01, -4.6123e-01,\n",
      "         5.1614e-01, -1.5620e-01,  1.2081e-01, -2.2050e-01,  1.4105e-01,\n",
      "        -8.5172e-01,  1.8914e-01,  8.4010e-01,  1.4209e-01, -1.6895e-01,\n",
      "         4.2435e-01,  8.9550e-02,  8.9112e-02,  4.2290e-01,  3.2373e-02,\n",
      "        -3.6291e-01,  1.3453e-01, -8.5668e-01, -2.4248e-01, -9.4522e-02,\n",
      "         1.2484e-01,  5.5027e-02,  4.1404e-01,  2.3981e-01, -8.2740e-02,\n",
      "        -1.1962e+00, -2.3321e-01, -4.1346e-02, -2.1395e-01,  4.9982e-02,\n",
      "        -1.7694e-01,  2.0223e-01, -1.1533e-01,  3.6442e-01,  1.2325e-01,\n",
      "        -6.3594e-01,  2.8268e-01, -6.7389e-01,  1.2171e+00,  8.9791e-02,\n",
      "         3.6282e-01, -4.4252e-01,  5.7637e-01, -3.5949e-01,  1.2628e-01,\n",
      "        -6.4305e-01, -7.0296e-01,  7.2355e-01,  2.5340e-01, -3.7389e-01,\n",
      "         2.3680e-01, -9.3948e-02, -2.4011e-01, -5.8613e-02,  3.4620e-01,\n",
      "        -2.0905e+00, -7.3129e-02,  9.4078e-01,  1.4330e-01, -1.1418e-01,\n",
      "        -5.2393e-01, -1.8653e-01,  1.3258e-01, -6.1935e-01,  2.4267e-01,\n",
      "         6.6479e-02, -1.9458e-02,  3.8424e-01, -6.7964e-01,  3.2507e-01,\n",
      "        -1.4325e-01,  5.3301e-02, -1.0027e-01, -1.5970e-01, -4.2886e-01,\n",
      "        -4.7330e-01,  5.9062e-01,  1.9253e-01,  7.2665e-02,  3.9477e-01,\n",
      "        -3.7150e-01, -3.7080e-01,  4.7081e-01, -2.0914e-01, -6.2440e-02,\n",
      "        -8.5916e-02, -4.3697e-01, -1.0338e+00, -3.5868e-01,  5.1572e-01,\n",
      "         3.5072e-01,  3.6954e-01,  1.5745e-01,  2.4417e-01,  5.4278e-01,\n",
      "        -5.5842e-01,  6.8706e-01,  7.5136e-01,  4.6463e-01,  5.8360e-01,\n",
      "         2.9817e-01, -1.6886e-01,  3.0656e-01,  2.0311e-01, -3.5293e-01,\n",
      "        -3.3876e-01, -3.2494e-01, -2.7274e-01, -2.9015e-01, -2.0023e-01,\n",
      "        -3.4201e-01, -3.7693e-02,  1.0649e-01, -3.5553e-01, -3.2270e-01,\n",
      "         2.0213e-01, -2.3848e-01, -4.8012e-01,  1.0761e-01, -3.3961e-01,\n",
      "        -8.2627e-01,  8.2876e-02, -6.5724e-01, -4.4849e-01,  5.4825e-01,\n",
      "         1.1082e+00,  5.1172e-01, -1.1053e+00,  6.8602e-03,  5.4167e-02,\n",
      "         2.2281e-01, -3.8139e-01,  1.8843e-02,  4.7144e-01, -7.6652e-01,\n",
      "         1.4440e-01, -5.0405e-01,  2.4719e-01,  3.6783e-01, -1.9595e-01,\n",
      "         7.2689e-01, -4.5527e-01, -3.2749e-01, -4.7187e-02, -4.7577e-02,\n",
      "        -5.1834e-01, -4.7684e-01,  2.5805e-01, -4.3171e-01,  4.1742e-01,\n",
      "        -3.1879e-02, -1.6256e-01,  2.2680e-01, -4.4143e-03,  1.2498e-03,\n",
      "        -9.6373e-02,  7.0514e-01,  6.4616e-01,  1.1405e-01,  7.0665e-01,\n",
      "         8.2815e-02,  5.7574e-01, -3.0116e-03, -3.2084e-01,  1.0788e-01,\n",
      "         8.9035e-02,  5.1887e-02, -2.8145e-01,  1.3646e-01,  2.9181e-01,\n",
      "        -5.0088e-01,  3.9828e-01, -4.3079e-01,  1.6192e+00, -5.4041e-02,\n",
      "        -2.0312e-01, -7.4117e-01,  9.0575e-01, -3.0155e-01,  1.5338e-02,\n",
      "         4.7468e-01, -3.1961e-01,  4.8722e-01, -6.1576e-01,  6.5599e-01,\n",
      "         1.9907e-02, -2.2383e-01,  8.2496e-01,  1.6823e-01,  3.2838e-01,\n",
      "        -4.7061e-01, -5.8621e-01, -1.6469e-01, -8.4057e-01,  3.2562e-01,\n",
      "         4.7197e-01, -6.3525e-02, -1.6029e-01,  3.5892e-01, -2.3216e-01,\n",
      "        -1.2495e-01,  3.0324e-01,  7.7459e-01, -9.0318e-01,  1.6723e-01,\n",
      "         5.1161e-01,  5.8852e-01, -7.1251e-01, -7.7395e-02,  5.6558e-02,\n",
      "        -1.4160e-01, -2.7935e-01,  1.1286e-01, -5.4542e-02, -3.3991e-01,\n",
      "         8.3968e-01, -5.1392e-01, -2.7452e-01,  7.8019e-01, -2.4997e-01,\n",
      "        -3.8041e-01,  4.7520e-01,  2.8394e-01,  5.3878e-02,  2.2415e-01,\n",
      "        -5.2637e-01, -1.4229e-01,  4.1019e-01, -3.7810e-01,  2.1355e-01,\n",
      "        -2.5680e-01, -3.6915e-01,  2.7287e-01, -5.5787e-01,  4.6236e-01,\n",
      "         3.7592e-01, -1.8410e-01, -3.4780e-01,  2.6267e-01, -3.3620e-01,\n",
      "         2.5308e-01,  3.4323e-01, -4.5262e-01, -7.1657e-03,  1.4033e-01,\n",
      "         3.3490e-02,  2.6176e-01,  6.4814e-01,  9.3990e-01,  3.5137e-01,\n",
      "        -6.3644e-01, -8.2094e-01, -1.9804e+00,  5.4776e-01,  1.8100e-01,\n",
      "         2.5905e-01,  1.4530e-01, -4.1955e-02,  4.5506e-01, -3.1959e-02,\n",
      "         3.6156e-03,  2.3001e-01, -1.0261e-01,  3.1115e-01,  5.9139e-01,\n",
      "        -3.8738e-01,  8.9866e-02, -1.8607e-01,  7.9103e-01,  5.5973e-02,\n",
      "        -6.9419e-02, -2.5552e-01,  1.3806e-01,  3.6831e-01, -5.0641e-01,\n",
      "        -6.6977e-01, -6.3900e-01,  2.3745e-01,  8.5872e-01, -3.8571e-01,\n",
      "         2.2172e-01,  5.4704e-01, -3.2829e-01,  3.6617e-01, -5.4716e-01,\n",
      "        -1.6500e-01,  2.8436e-01, -8.8678e-02, -3.3967e-01,  2.1754e-01,\n",
      "        -1.7754e-01,  2.6476e-01, -3.5379e-01,  8.1598e-01, -4.7148e-01,\n",
      "         3.7138e-01,  5.1082e-01, -6.8235e-01,  6.9563e-01, -1.2545e-01,\n",
      "         1.9147e-01, -7.6718e-01, -3.0732e-01, -3.0803e-02,  3.9063e-01,\n",
      "        -1.9565e-01,  3.2219e-01, -2.6340e-01,  2.5722e-01,  3.6575e-01,\n",
      "         2.3402e-02, -7.8015e-01, -5.2430e-01, -1.5335e-01, -2.9439e-01,\n",
      "        -2.3025e-01,  7.9498e-01, -6.4119e-02, -2.2547e-01, -1.1309e-01,\n",
      "        -3.3698e-01, -3.2996e-01, -4.9369e-01, -3.3414e-02,  2.9670e-01,\n",
      "         8.8496e-02,  2.6333e-01, -3.1652e-02,  3.9059e-01,  4.6393e-01,\n",
      "         9.4876e-01, -1.6366e-01, -2.6991e-02, -1.0282e-01,  4.5183e-02,\n",
      "        -1.5349e-01,  2.2958e-02, -5.4718e+00, -1.9011e-01, -3.3873e-01,\n",
      "        -3.9588e-01, -3.2821e-01, -3.2970e-01,  2.8015e-02, -3.6785e-01,\n",
      "        -1.4213e-01, -5.5940e-01, -1.3452e-01,  5.0493e-01, -1.8361e-01,\n",
      "        -1.1542e-01,  5.3940e-01,  1.4558e-01]), 0)\n"
     ]
    }
   ],
   "source": [
    "# show the first element\n",
    "print(f\"Dataset length: {len(new_dataset)}\")\n",
    "print(f\"Dataset first element: {new_dataset[0]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
