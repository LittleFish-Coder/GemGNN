{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 4487/4487 [00:00<00:00, 35809.87 examples/s]\n",
      "Generating test split: 100%|██████████| 499/499 [00:00<00:00, 34915.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# load data\n",
    "dataset: DatasetDict = load_dataset(\"LittleFish-Coder/Fake_News_KDD2020\", download_mode=\"reuse_cache_if_exists\", cache_dir=\"dataset\")   # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'embeddings', 'label'],\n",
      "        num_rows: 4487\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'embeddings', 'label'],\n",
      "        num_rows: 499\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "print(f\"Dataset: {dataset}\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First training sample\n",
      "Keys: dict_keys(['text', 'embeddings', 'label'])\n",
      "Text: Oops. Something went wrong. Please try again later  Looks like we are having a problem on the server.\n",
      "Label: 0\n"
     ]
    }
   ],
   "source": [
    "# quick look at the data\n",
    "first_train = train_dataset[0]\n",
    "print(f\"First training sample\")\n",
    "print(f\"Keys: {first_train.keys()}\")\n",
    "print(f\"Text: {first_train['text']}\")\n",
    "print(f\"Label: {first_train['label']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Reports of Lawrence and Pitt dating spread in December 2017. Watch What Happens Live/YouTube and Jason Kempin/Getty Images for Netflix  Jennifer Lawrence appeared on Bravo's \"Watch What Happens Live\" on Thursday, March 1 and responded to the reports that she was dating Brad Pitt.  When a caller asked her if the two stars were \"secretly dating,\" the \"Red Sparrow\" actress denied it.  Even though she says the reports weren't true, Lawrence admitted that she didn't mind the speculation too much.  \"No,\" she said. \"I've met him once in like, 2013, so it was very random, but I also wasn't like, in a hurry to debunk it.\"  In December 2017, it was speculated that Lawrence and Pitt were dating.  In December 2017, it was reported that Jennifer Lawrence was dating Brad Pitt, but the \"Red Sparrow\" actress has now cleared up the speculation and revealed it's not true.  While appearing on Bravo's \"Watch What Happens Live\" on Thursday, Lawrence answered questions from fans who called in, and one viewer from Pennsylvania asked if the reports of her dating Pitt were true. Lawrence denied them, and explained that they only met once a few years ago.  Even though the speculation was false, she admitted that she didn't mind being paired with Pitt.  \"No,\" she said. \"I've met him once in like, 2013, so it was very random, but I also wasn't like, in a hurry to debunk it.\"  Speculation of Lawrence and Pitt dating surfaced in December 2017, with reports stating that Pitt \"had his eye on Jennifer for years\" and they were \"enjoying lots of late nights together.\"  Lawrence has been single since she split with Darren Aronofsky, who directed \"Mother!,\" in 2017 and Pitt has been single since he split with Angelina Jolie in 2016.  Lawrence also isn't the first actor to be linked to Pitt. In November 2017, Kate Hudson appeared on \"Watch What Happens Live\" and addressed the reports that she was dating Pitt. Like Lawrence, she didn't seem to mind the report and called it \"kind of awesome.\"  \"I kind of liked it,\" Hudson said. \"I was like, 'OK, fine. We're having twins!'\"  Sign up here to get INSIDER's favorite stories straight to your inbox.\n"
     ]
    }
   ],
   "source": [
    "text = test_dataset[0][\"text\"]\n",
    "print(f\"Text: {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Directly Text Classification (Pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'kdd2020'    # ['fake-news-tfg', 'kdd2020']\n",
    "model_name = 'distilbert-base-uncased'  # ['bert-base-uncased', 'distilbert-base-uncased', 'roberta-base']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "# get the model from huggingface model hub\n",
    "pipe = pipeline(\"text-classification\", model=f\"LittleFish-Coder/{model_name}-{dataset_name}\", truncation=True, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'label': 'fake', 'score': 0.7897346615791321}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizer and Pretrained-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"LittleFish-Coder/{model_name}-{dataset_name}\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(f\"LittleFish-Coder/{model_name}-{dataset_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict via tokenizer & model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text and get the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: {'input_ids': tensor([[  101,  4311,  1997,  5623,  1998, 15091,  5306,  3659,  1999,  2285,\n",
      "          2418,  1012,  3422,  2054,  6433,  2444,  1013,  7858,  1998,  4463,\n",
      "         20441,  2378,  1013,  2131,  3723,  4871,  2005, 20907,  7673,  5623,\n",
      "          2596,  2006, 17562,  1005,  1055,  1000,  3422,  2054,  6433,  2444,\n",
      "          1000,  2006,  9432,  1010,  2233,  1015,  1998,  5838,  2000,  1996,\n",
      "          4311,  2008,  2016,  2001,  5306,  8226, 15091,  1012,  2043,  1037,\n",
      "         20587,  2356,  2014,  2065,  1996,  2048,  3340,  2020,  1000, 10082,\n",
      "          5306,  1010,  1000,  1996,  1000,  2417, 19479,  1000,  3883,  6380,\n",
      "          2009,  1012,  2130,  2295,  2016,  2758,  1996,  4311,  4694,  1005,\n",
      "          1056,  2995,  1010,  5623,  4914,  2008,  2016,  2134,  1005,  1056,\n",
      "          2568,  1996, 12143,  2205,  2172,  1012,  1000,  2053,  1010,  1000,\n",
      "          2016,  2056,  1012,  1000,  1045,  1005,  2310,  2777,  2032,  2320,\n",
      "          1999,  2066,  1010,  2286,  1010,  2061,  2009,  2001,  2200,  6721,\n",
      "          1010,  2021,  1045,  2036,  2347,  1005,  1056,  2066,  1010,  1999,\n",
      "          1037,  9241,  2000,  2139,  8569,  8950,  2009,  1012,  1000,  1999,\n",
      "          2285,  2418,  1010,  2009,  2001, 15520,  2008,  5623,  1998, 15091,\n",
      "          2020,  5306,  1012,  1999,  2285,  2418,  1010,  2009,  2001,  2988,\n",
      "          2008,  7673,  5623,  2001,  5306,  8226, 15091,  1010,  2021,  1996,\n",
      "          1000,  2417, 19479,  1000,  3883,  2038,  2085,  5985,  2039,  1996,\n",
      "         12143,  1998,  3936,  2009,  1005,  1055,  2025,  2995,  1012,  2096,\n",
      "          6037,  2006, 17562,  1005,  1055,  1000,  3422,  2054,  6433,  2444,\n",
      "          1000,  2006,  9432,  1010,  5623,  4660,  3980,  2013,  4599,  2040,\n",
      "          2170,  1999,  1010,  1998,  2028, 13972,  2013,  3552,  2356,  2065,\n",
      "          1996,  4311,  1997,  2014,  5306, 15091,  2020,  2995,  1012,  5623,\n",
      "          6380,  2068,  1010,  1998,  4541,  2008,  2027,  2069,  2777,  2320,\n",
      "          1037,  2261,  2086,  3283,  1012,  2130,  2295,  1996, 12143,  2001,\n",
      "          6270,  1010,  2016,  4914,  2008,  2016,  2134,  1005,  1056,  2568,\n",
      "          2108, 12739,  2007, 15091,  1012,  1000,  2053,  1010,  1000,  2016,\n",
      "          2056,  1012,  1000,  1045,  1005,  2310,  2777,  2032,  2320,  1999,\n",
      "          2066,  1010,  2286,  1010,  2061,  2009,  2001,  2200,  6721,  1010,\n",
      "          2021,  1045,  2036,  2347,  1005,  1056,  2066,  1010,  1999,  1037,\n",
      "          9241,  2000,  2139,  8569,  8950,  2009,  1012,  1000, 12143,  1997,\n",
      "          5623,  1998, 15091,  5306, 15791,  1999,  2285,  2418,  1010,  2007,\n",
      "          4311,  5517,  2008, 15091,  1000,  2018,  2010,  3239,  2006,  7673,\n",
      "          2005,  2086,  1000,  1998,  2027,  2020,  1000,  9107,  7167,  1997,\n",
      "          2397,  6385,  2362,  1012,  1000,  5623,  2038,  2042,  2309,  2144,\n",
      "          2016,  3975,  2007, 12270, 12098, 17175, 10343,  4801,  1010,  2040,\n",
      "          2856,  1000,  2388,   999,  1010,  1000,  1999,  2418,  1998, 15091,\n",
      "          2038,  2042,  2309,  2144,  2002,  3975,  2007, 23847,  8183,  8751,\n",
      "          1999,  2355,  1012,  5623,  2036,  3475,  1005,  1056,  1996,  2034,\n",
      "          3364,  2000,  2022,  5799,  2000, 15091,  1012,  1999,  2281,  2418,\n",
      "          1010,  5736,  6842,  2596,  2006,  1000,  3422,  2054,  6433,  2444,\n",
      "          1000,  1998,  8280,  1996,  4311,  2008,  2016,  2001,  5306, 15091,\n",
      "          1012,  2066,  5623,  1010,  2016,  2134,  1005,  1056,  4025,  2000,\n",
      "          2568,  1996,  3189,  1998,  2170,  2009,  1000,  2785,  1997, 12476,\n",
      "          1012,  1000,  1000,  1045,  2785,  1997,  4669,  2009,  1010,  1000,\n",
      "          6842,  2056,  1012,  1000,  1045,  2001,  2066,  1010,  1005,  7929,\n",
      "          1010,  2986,  1012,  2057,  1005,  2128,  2383,  8178,   999,  1005,\n",
      "          1000,  3696,  2039,  2182,  2000,  2131, 25297,  1005,  1055,  5440,\n",
      "          3441,  3442,  2000,  2115,  1999,  8758,  1012,   102]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Input: {inputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: SequenceClassifierOutput(loss=None, logits=tensor([[-0.8020,  0.5214]]), hidden_states=None, attentions=None)\n",
      "Logits: tensor([[-0.8020,  0.5214]])\n",
      "Prediction: fake\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_class_id = logits.argmax().item()\n",
    "prediction = model.config.id2label[predicted_class_id]\n",
    "print(f\"Output: {outputs}\")\n",
    "print(f\"Logits: {logits}\")\n",
    "print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the embedding of the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.output_hidden_states = True\n",
    "\n",
    "# Get model output with hidden states\n",
    "with torch.no_grad():  # Disable gradient calculation for inference\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Now, outputs will have the hidden states\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "# The last layer's hidden state can be accessed like this\n",
    "last_hidden_state = hidden_states[-1]\n",
    "\n",
    "# get the cls token embedding\n",
    "cls_embeddings = last_hidden_state[:, 0, :]   # (1, 768)\n",
    "\n",
    "# flatten the embeddings\n",
    "cls_embeddings = cls_embeddings.flatten()   # (768,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden states type: <class 'tuple'>\n",
      "hidden states Length: 7\n"
     ]
    }
   ],
   "source": [
    "print(f\"hidden states type: {type(hidden_states)}\")\n",
    "print(f\"hidden states Length: {len(hidden_states)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state type: <class 'torch.Tensor'>\n",
      "last hidden state shape: torch.Size([1, 498, 768])\n"
     ]
    }
   ],
   "source": [
    "print(f\"last hidden state type: {type(last_hidden_state)}\")\n",
    "print(f\"last hidden state shape: {last_hidden_state.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls embeddings type: <class 'torch.Tensor'>\n",
      "cls embeddings shape: torch.Size([768])\n",
      "cls embeddings: tensor([ 0.7229, -0.4721, -0.5581, -0.1805,  0.0620,  0.3655,  0.3687,  0.8344,\n",
      "        -0.2113, -0.0896, -0.1241,  0.5803, -0.4522,  1.1088,  0.6336, -0.0772,\n",
      "        -0.7328,  0.0427, -0.8334,  0.3931,  0.3962,  0.4526,  0.1150, -0.4167,\n",
      "        -0.5893, -0.2429,  0.6974, -0.2206,  0.3283,  0.0179,  0.0660, -0.0030,\n",
      "         0.0931,  1.0225, -0.9738, -0.4682, -0.5479,  0.5818, -0.0042, -0.7241,\n",
      "        -0.0964, -1.0804,  0.0816, -0.1570,  0.0300,  0.6269, -2.5275,  1.3813,\n",
      "        -0.3521,  0.4846,  0.6566,  1.1796,  0.0407,  1.1897,  1.3038,  1.0105,\n",
      "        -0.4753, -1.2677, -0.7465, -0.0726, -0.0122,  0.3052,  0.1873, -1.2204,\n",
      "         0.4671, -0.8569, -0.2887,  0.9403,  0.4929, -0.8999, -0.1070,  0.0155,\n",
      "        -0.5269,  0.3997, -0.6080,  0.2152, -0.1855,  0.8197, -0.3379, -0.0529,\n",
      "        -0.0265,  0.0366, -0.0286,  1.3668, -0.5931, -0.6238, -0.1626,  0.2438,\n",
      "        -0.5348,  1.4884, -1.6267,  0.4665,  0.2794,  0.4028,  0.0868, -0.1074,\n",
      "        -0.1049, -0.1928,  0.2806, -0.4760, -0.0722, -0.4317,  0.1425, -0.5865,\n",
      "         0.8190, -1.0822,  0.6807, -0.6462, -0.0633,  0.6887,  0.0962,  0.6043,\n",
      "         0.5487, -0.9743, -0.9073, -0.2801, -1.2922, -0.5805, -1.1031, -0.1538,\n",
      "         1.3823, -0.0403, -0.8500, -0.3689,  0.0203,  0.9903,  0.3940,  0.1006,\n",
      "        -1.2438,  0.1736,  1.0248,  0.4040, -0.5190,  0.5015, -1.0776,  0.2382,\n",
      "         0.0577, -0.7533,  0.2809,  0.0435,  0.7610, -0.4983, -1.1674,  0.5994,\n",
      "         0.5005, -0.1735, -0.0261,  0.5915, -0.5567, -0.6660, -0.8492, -0.7391,\n",
      "         1.2925, -1.3134, -0.6595,  0.3335,  0.0254,  0.6427, -0.8045,  0.2441,\n",
      "         0.5746, -0.4834, -0.0462,  0.2209,  0.5714,  1.0518,  0.2240,  0.1506,\n",
      "        -0.9339,  0.6495, -0.1502, -0.3413,  1.6437, -0.6400,  0.3409,  0.4961,\n",
      "        -0.4849,  0.4047,  0.7932,  1.1661, -0.6867,  0.4159, -0.2298,  0.3832,\n",
      "         0.3424, -0.7149, -0.2456, -0.1392,  0.5104,  0.3694, -1.0881, -0.3364,\n",
      "        -0.0667, -0.2242,  0.0714, -0.4997,  0.5358,  1.0044, -0.9246,  0.1312,\n",
      "        -0.0660,  0.3392,  0.9414, -0.6976, -1.3122,  0.3053,  0.1674, -0.8590,\n",
      "        -0.5973,  0.1246, -0.6656,  0.2159,  0.2737, -0.0245, -0.4639,  0.3034,\n",
      "        -0.4646, -0.6139,  0.1402, -0.4031, -0.5067,  0.3741, -0.2531, -0.2870,\n",
      "         0.7108,  0.9274, -0.2402, -0.3757,  0.8190, -0.2416, -0.3416,  0.9318,\n",
      "         0.7351,  0.8930, -0.3407, -0.3674, -0.3087, -1.5800, -0.1652, -0.8335,\n",
      "        -0.5067,  0.5431, -0.2101,  0.4788, -0.4584,  0.4327,  0.0267, -0.4159,\n",
      "        -0.3660, -0.3780,  0.2817,  0.5164, -0.6043, -0.0708,  0.3978, -0.3738,\n",
      "        -0.4382, -0.3891, -0.0412,  0.5081, -0.0434, -0.8132, -0.1574,  0.4040,\n",
      "        -0.3165, -0.7478, -0.8547,  0.1358,  0.3423, -0.0521,  0.3994,  0.0082,\n",
      "        -0.5145,  1.5842, -0.0976,  0.1786,  0.1409,  0.2116,  0.5475, -1.1333,\n",
      "         0.2253, -0.0525, -1.1374,  0.1274, -0.0095, -0.0609, -1.1976, -1.6592,\n",
      "         0.0436,  0.2102,  0.0790,  0.4974,  0.0760, -0.7244, -0.4899, -0.5523,\n",
      "         0.3352,  1.2339,  0.1019, -0.7876, -1.0959,  1.0507,  0.7011, -0.1965,\n",
      "         0.5359,  0.0314,  0.1134, -0.8767,  2.0766, -0.2988,  0.4501, -0.9288,\n",
      "        -0.1254,  0.5574,  0.0727,  0.2565, -0.0934, -0.2056,  0.8479, -0.1152,\n",
      "         0.1164,  0.6070,  0.3283,  0.3225,  1.0102,  0.1220, -0.1216,  0.1622,\n",
      "        -0.0778,  0.0043,  0.0807,  0.3046,  0.7624, -0.1247, -0.9665, -0.5550,\n",
      "         0.7862, -0.9785, -0.4431, -1.0665,  0.1798,  0.6072, -0.1798,  0.6100,\n",
      "        -0.1077, -0.9497,  0.3720,  0.1025,  0.5543,  0.1884, -0.6002,  0.2471,\n",
      "         0.3588,  0.0607,  0.5457,  1.1012, -0.5632,  0.1181,  0.4120,  0.6335,\n",
      "         0.4055,  0.2429,  0.1839,  0.4714,  0.2321, -0.1398, -0.6108, -0.1080,\n",
      "        -0.0373,  0.3033, -0.1230, -0.5765,  0.4501, -0.8625, -0.5413,  0.3291,\n",
      "         0.0974, -0.9943, -0.7882,  0.9771, -0.1328,  0.8848,  0.3087, -0.6553,\n",
      "         0.2250, -0.2857, -0.0036,  0.5763, -0.1870, -0.1355,  0.2059,  0.4827,\n",
      "        -0.5469,  0.4837, -0.0806, -0.0792,  0.3720, -1.1025, -0.2267, -0.5450,\n",
      "        -0.2139,  0.6769, -0.0836,  0.0549, -0.3748, -0.5711,  0.1777,  0.4983,\n",
      "        -0.7797, -0.8401,  0.3620, -0.2622, -1.0139, -0.0065, -0.5643, -0.1579,\n",
      "        -0.2730, -0.0673,  0.2545, -0.3739,  0.4159,  0.3888,  0.5391, -0.4214,\n",
      "         0.7109,  0.0753,  1.0227, -1.1139,  0.6323, -0.5994, -0.0736,  0.7419,\n",
      "        -0.1270, -0.2616, -0.4054,  0.0491, -0.2125, -0.0974, -0.4486,  0.0668,\n",
      "        -0.3406, -0.2698, -0.0490,  0.0540, -0.0785,  0.0264,  0.0105,  0.4812,\n",
      "         0.0669,  0.2278,  0.2960, -0.0703, -0.0185, -0.0546,  0.8943,  0.4561,\n",
      "        -1.4970,  0.5243, -0.2480,  1.1174,  0.3644,  0.0233,  0.6808, -0.3779,\n",
      "        -0.2195, -0.0114, -0.3437, -0.9149,  0.4381, -0.0978, -1.9115,  0.0298,\n",
      "        -0.7831,  0.2236,  0.0872,  0.3731,  0.8633,  0.6420,  0.3665, -0.2958,\n",
      "         0.3965,  0.0100,  0.1020, -0.3955, -0.4038, -0.4272, -0.8232, -0.3505,\n",
      "        -0.6901,  0.1611, -0.9092,  0.2176, -0.5648,  0.2337, -0.1297,  0.2091,\n",
      "        -0.7035,  0.3416,  0.7782,  0.8266,  0.2143, -0.0090, -0.5520, -0.1839,\n",
      "        -0.0927, -0.4995,  0.0327, -0.5579,  0.5423,  0.0555, -1.0462, -0.1285,\n",
      "         0.7240,  0.8398,  0.5412,  0.4378,  0.1534, -0.2130, -0.4961,  0.6607,\n",
      "         0.0841, -0.2837,  0.4488,  0.0866, -0.5110,  0.3123, -0.6757, -0.0557,\n",
      "        -0.8410,  0.0097,  0.7610, -0.0634, -0.2826, -0.2982,  0.3835, -0.0109,\n",
      "        -0.6817, -0.8609, -0.1326, -0.9033,  0.2604,  0.9600, -0.1765, -0.2556,\n",
      "        -0.2833, -0.5304, -0.3163, -0.1471, -0.2676, -1.2348,  0.0407,  0.1595,\n",
      "        -0.0671,  0.7009, -0.0404,  0.6778, -0.8785,  0.2965, -0.2111, -0.4566,\n",
      "        -0.2005,  0.3709, -0.9894, -0.2947,  0.2010, -0.8397, -1.4699, -0.4667,\n",
      "        -0.1910, -0.2022,  0.6052, -1.0665,  0.6691, -0.0945,  0.0829, -0.0779,\n",
      "         0.6218, -0.0246,  0.0166,  0.0604,  1.1496,  0.2260,  0.6483,  0.9708,\n",
      "        -0.5466, -0.8001, -0.7819, -0.0633,  0.0283,  0.6409, -1.0388, -0.3332,\n",
      "        -0.6188, -0.5812,  0.5053,  0.0746,  0.4659, -0.5955,  0.0941,  0.2423,\n",
      "         0.2102, -0.3191,  0.0239, -0.1614,  0.4084, -0.3519, -0.6894, -0.5157,\n",
      "        -0.5717, -0.1447,  0.0755,  0.3342, -0.6963,  0.9826, -0.3357,  0.2875,\n",
      "        -0.1438, -0.4431,  0.0213,  0.2507,  1.3594, -0.1438, -1.0957,  0.1482,\n",
      "         0.0349, -0.0183, -0.3988, -0.0691,  0.1748,  0.0793,  0.1419, -1.0392,\n",
      "         0.5531, -0.2002,  0.4716, -0.1767,  0.4814,  0.8010, -0.6970,  0.2320,\n",
      "         0.4828,  0.3253,  0.6002, -0.6614,  0.4914,  0.6494,  0.3211,  1.4938,\n",
      "        -0.1883, -0.2796, -0.1638, -0.3507, -0.8448,  0.4280,  0.4064, -0.3288,\n",
      "         0.4487,  0.1770, -0.0609,  0.7188, -0.1682,  0.5856,  0.8253,  0.3910,\n",
      "         0.6872,  0.8653, -0.6768,  0.4274,  0.1006,  0.9178,  0.5785, -0.5098,\n",
      "        -0.0559,  0.5550,  0.3114,  0.5451,  0.0346, -0.5397,  0.7421,  0.6155,\n",
      "        -0.2084,  0.2175, -0.0943, -1.2910,  0.4624,  0.0715, -0.6029,  0.7594,\n",
      "        -0.4789,  0.0422, -0.3175, -0.3879, -0.1517, -0.0076, -0.2734, -0.1902,\n",
      "         0.3060,  0.4947, -0.2415, -0.4846,  0.5735,  0.7599, -0.5873, -0.9110,\n",
      "         0.0538, -0.7437, -0.3443,  0.0096,  0.6391, -0.1165, -0.3377, -0.0257,\n",
      "         0.1595,  0.0949, -0.0439, -0.3208, -0.4799,  0.3458, -0.3598,  0.0054,\n",
      "        -0.8692, -0.0067, -1.1156, -0.1281,  0.5425,  0.0404,  0.0546, -0.7368,\n",
      "         1.3088, -0.8557,  0.4428,  0.7235, -0.4539, -1.0998, -0.6645,  0.4969,\n",
      "        -0.7037,  0.4230, -1.1653,  1.1786,  0.6105, -0.0166, -0.9500,  0.3372,\n",
      "         0.5717,  0.5302, -1.6760, -0.1979,  0.2240, -0.2421, -0.6272, -0.9270,\n",
      "        -0.4358,  0.0583,  0.1724,  0.3030, -0.0714,  0.2315, -0.6767,  0.7164,\n",
      "        -0.3751, -0.2129, -0.4595, -0.5329, -0.4181,  0.4426,  0.2504,  0.2264])\n"
     ]
    }
   ],
   "source": [
    "print(f\"cls embeddings type: {type(cls_embeddings)}\")\n",
    "print(f\"cls embeddings shape: {cls_embeddings.shape}\")\n",
    "print(f\"cls embeddings: {cls_embeddings}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'kdd2020'\n",
    "model_name = 'distilbert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(f\"LittleFish-Coder/{model_name}-{dataset_name}\")\n",
    "model = AutoModel.from_pretrained(f\"LittleFish-Coder/{model_name}-{dataset_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_embeddings = model(**inputs).last_hidden_state[:, 0, :].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cls embeddings shape: torch.Size([768])\n",
      "cls embeddings: tensor([ 0.7229, -0.4721, -0.5581, -0.1805,  0.0620,  0.3655,  0.3687,  0.8344,\n",
      "        -0.2113, -0.0896, -0.1241,  0.5803, -0.4522,  1.1088,  0.6336, -0.0772,\n",
      "        -0.7328,  0.0427, -0.8334,  0.3931,  0.3962,  0.4526,  0.1150, -0.4167,\n",
      "        -0.5893, -0.2429,  0.6974, -0.2206,  0.3283,  0.0179,  0.0660, -0.0030,\n",
      "         0.0931,  1.0225, -0.9738, -0.4682, -0.5479,  0.5818, -0.0042, -0.7241,\n",
      "        -0.0964, -1.0804,  0.0816, -0.1570,  0.0300,  0.6269, -2.5275,  1.3813,\n",
      "        -0.3521,  0.4846,  0.6566,  1.1796,  0.0407,  1.1897,  1.3038,  1.0105,\n",
      "        -0.4753, -1.2677, -0.7465, -0.0726, -0.0122,  0.3052,  0.1873, -1.2204,\n",
      "         0.4671, -0.8569, -0.2887,  0.9403,  0.4929, -0.8999, -0.1070,  0.0155,\n",
      "        -0.5269,  0.3997, -0.6080,  0.2152, -0.1855,  0.8197, -0.3379, -0.0529,\n",
      "        -0.0265,  0.0366, -0.0286,  1.3668, -0.5931, -0.6238, -0.1626,  0.2438,\n",
      "        -0.5348,  1.4884, -1.6267,  0.4665,  0.2794,  0.4028,  0.0868, -0.1074,\n",
      "        -0.1049, -0.1928,  0.2806, -0.4760, -0.0722, -0.4317,  0.1425, -0.5865,\n",
      "         0.8190, -1.0822,  0.6807, -0.6462, -0.0633,  0.6887,  0.0962,  0.6043,\n",
      "         0.5487, -0.9743, -0.9073, -0.2801, -1.2922, -0.5805, -1.1031, -0.1538,\n",
      "         1.3823, -0.0403, -0.8500, -0.3689,  0.0203,  0.9903,  0.3940,  0.1006,\n",
      "        -1.2438,  0.1736,  1.0248,  0.4040, -0.5190,  0.5015, -1.0776,  0.2382,\n",
      "         0.0577, -0.7533,  0.2809,  0.0435,  0.7610, -0.4983, -1.1674,  0.5994,\n",
      "         0.5005, -0.1735, -0.0261,  0.5915, -0.5567, -0.6660, -0.8492, -0.7391,\n",
      "         1.2925, -1.3134, -0.6595,  0.3335,  0.0254,  0.6427, -0.8045,  0.2441,\n",
      "         0.5746, -0.4834, -0.0462,  0.2209,  0.5714,  1.0518,  0.2240,  0.1506,\n",
      "        -0.9339,  0.6495, -0.1502, -0.3413,  1.6437, -0.6400,  0.3409,  0.4961,\n",
      "        -0.4849,  0.4047,  0.7932,  1.1661, -0.6867,  0.4159, -0.2298,  0.3832,\n",
      "         0.3424, -0.7149, -0.2456, -0.1392,  0.5104,  0.3694, -1.0881, -0.3364,\n",
      "        -0.0667, -0.2242,  0.0714, -0.4997,  0.5358,  1.0044, -0.9246,  0.1312,\n",
      "        -0.0660,  0.3392,  0.9414, -0.6976, -1.3122,  0.3053,  0.1674, -0.8590,\n",
      "        -0.5973,  0.1246, -0.6656,  0.2159,  0.2737, -0.0245, -0.4639,  0.3034,\n",
      "        -0.4646, -0.6139,  0.1402, -0.4031, -0.5067,  0.3741, -0.2531, -0.2870,\n",
      "         0.7108,  0.9274, -0.2402, -0.3757,  0.8190, -0.2416, -0.3416,  0.9318,\n",
      "         0.7351,  0.8930, -0.3407, -0.3674, -0.3087, -1.5800, -0.1652, -0.8335,\n",
      "        -0.5067,  0.5431, -0.2101,  0.4788, -0.4584,  0.4327,  0.0267, -0.4159,\n",
      "        -0.3660, -0.3780,  0.2817,  0.5164, -0.6043, -0.0708,  0.3978, -0.3738,\n",
      "        -0.4382, -0.3891, -0.0412,  0.5081, -0.0434, -0.8132, -0.1574,  0.4040,\n",
      "        -0.3165, -0.7478, -0.8547,  0.1358,  0.3423, -0.0521,  0.3994,  0.0082,\n",
      "        -0.5145,  1.5842, -0.0976,  0.1786,  0.1409,  0.2116,  0.5475, -1.1333,\n",
      "         0.2253, -0.0525, -1.1374,  0.1274, -0.0095, -0.0609, -1.1976, -1.6592,\n",
      "         0.0436,  0.2102,  0.0790,  0.4974,  0.0760, -0.7244, -0.4899, -0.5523,\n",
      "         0.3352,  1.2339,  0.1019, -0.7876, -1.0959,  1.0507,  0.7011, -0.1965,\n",
      "         0.5359,  0.0314,  0.1134, -0.8767,  2.0766, -0.2988,  0.4501, -0.9288,\n",
      "        -0.1254,  0.5574,  0.0727,  0.2565, -0.0934, -0.2056,  0.8479, -0.1152,\n",
      "         0.1164,  0.6070,  0.3283,  0.3225,  1.0102,  0.1220, -0.1216,  0.1622,\n",
      "        -0.0778,  0.0043,  0.0807,  0.3046,  0.7624, -0.1247, -0.9665, -0.5550,\n",
      "         0.7862, -0.9785, -0.4431, -1.0665,  0.1798,  0.6072, -0.1798,  0.6100,\n",
      "        -0.1077, -0.9497,  0.3720,  0.1025,  0.5543,  0.1884, -0.6002,  0.2471,\n",
      "         0.3588,  0.0607,  0.5457,  1.1012, -0.5632,  0.1181,  0.4120,  0.6335,\n",
      "         0.4055,  0.2429,  0.1839,  0.4714,  0.2321, -0.1398, -0.6108, -0.1080,\n",
      "        -0.0373,  0.3033, -0.1230, -0.5765,  0.4501, -0.8625, -0.5413,  0.3291,\n",
      "         0.0974, -0.9943, -0.7882,  0.9771, -0.1328,  0.8848,  0.3087, -0.6553,\n",
      "         0.2250, -0.2857, -0.0036,  0.5763, -0.1870, -0.1355,  0.2059,  0.4827,\n",
      "        -0.5469,  0.4837, -0.0806, -0.0792,  0.3720, -1.1025, -0.2267, -0.5450,\n",
      "        -0.2139,  0.6769, -0.0836,  0.0549, -0.3748, -0.5711,  0.1777,  0.4983,\n",
      "        -0.7797, -0.8401,  0.3620, -0.2622, -1.0139, -0.0065, -0.5643, -0.1579,\n",
      "        -0.2730, -0.0673,  0.2545, -0.3739,  0.4159,  0.3888,  0.5391, -0.4214,\n",
      "         0.7109,  0.0753,  1.0227, -1.1139,  0.6323, -0.5994, -0.0736,  0.7419,\n",
      "        -0.1270, -0.2616, -0.4054,  0.0491, -0.2125, -0.0974, -0.4486,  0.0668,\n",
      "        -0.3406, -0.2698, -0.0490,  0.0540, -0.0785,  0.0264,  0.0105,  0.4812,\n",
      "         0.0669,  0.2278,  0.2960, -0.0703, -0.0185, -0.0546,  0.8943,  0.4561,\n",
      "        -1.4970,  0.5243, -0.2480,  1.1174,  0.3644,  0.0233,  0.6808, -0.3779,\n",
      "        -0.2195, -0.0114, -0.3437, -0.9149,  0.4381, -0.0978, -1.9115,  0.0298,\n",
      "        -0.7831,  0.2236,  0.0872,  0.3731,  0.8633,  0.6420,  0.3665, -0.2958,\n",
      "         0.3965,  0.0100,  0.1020, -0.3955, -0.4038, -0.4272, -0.8232, -0.3505,\n",
      "        -0.6901,  0.1611, -0.9092,  0.2176, -0.5648,  0.2337, -0.1297,  0.2091,\n",
      "        -0.7035,  0.3416,  0.7782,  0.8266,  0.2143, -0.0090, -0.5520, -0.1839,\n",
      "        -0.0927, -0.4995,  0.0327, -0.5579,  0.5423,  0.0555, -1.0462, -0.1285,\n",
      "         0.7240,  0.8398,  0.5412,  0.4378,  0.1534, -0.2130, -0.4961,  0.6607,\n",
      "         0.0841, -0.2837,  0.4488,  0.0866, -0.5110,  0.3123, -0.6757, -0.0557,\n",
      "        -0.8410,  0.0097,  0.7610, -0.0634, -0.2826, -0.2982,  0.3835, -0.0109,\n",
      "        -0.6817, -0.8609, -0.1326, -0.9033,  0.2604,  0.9600, -0.1765, -0.2556,\n",
      "        -0.2833, -0.5304, -0.3163, -0.1471, -0.2676, -1.2348,  0.0407,  0.1595,\n",
      "        -0.0671,  0.7009, -0.0404,  0.6778, -0.8785,  0.2965, -0.2111, -0.4566,\n",
      "        -0.2005,  0.3709, -0.9894, -0.2947,  0.2010, -0.8397, -1.4699, -0.4667,\n",
      "        -0.1910, -0.2022,  0.6052, -1.0665,  0.6691, -0.0945,  0.0829, -0.0779,\n",
      "         0.6218, -0.0246,  0.0166,  0.0604,  1.1496,  0.2260,  0.6483,  0.9708,\n",
      "        -0.5466, -0.8001, -0.7819, -0.0633,  0.0283,  0.6409, -1.0388, -0.3332,\n",
      "        -0.6188, -0.5812,  0.5053,  0.0746,  0.4659, -0.5955,  0.0941,  0.2423,\n",
      "         0.2102, -0.3191,  0.0239, -0.1614,  0.4084, -0.3519, -0.6894, -0.5157,\n",
      "        -0.5717, -0.1447,  0.0755,  0.3342, -0.6963,  0.9826, -0.3357,  0.2875,\n",
      "        -0.1438, -0.4431,  0.0213,  0.2507,  1.3594, -0.1438, -1.0957,  0.1482,\n",
      "         0.0349, -0.0183, -0.3988, -0.0691,  0.1748,  0.0793,  0.1419, -1.0392,\n",
      "         0.5531, -0.2002,  0.4716, -0.1767,  0.4814,  0.8010, -0.6970,  0.2320,\n",
      "         0.4828,  0.3253,  0.6002, -0.6614,  0.4914,  0.6494,  0.3211,  1.4938,\n",
      "        -0.1883, -0.2796, -0.1638, -0.3507, -0.8448,  0.4280,  0.4064, -0.3288,\n",
      "         0.4487,  0.1770, -0.0609,  0.7188, -0.1682,  0.5856,  0.8253,  0.3910,\n",
      "         0.6872,  0.8653, -0.6768,  0.4274,  0.1006,  0.9178,  0.5785, -0.5098,\n",
      "        -0.0559,  0.5550,  0.3114,  0.5451,  0.0346, -0.5397,  0.7421,  0.6155,\n",
      "        -0.2084,  0.2175, -0.0943, -1.2910,  0.4624,  0.0715, -0.6029,  0.7594,\n",
      "        -0.4789,  0.0422, -0.3175, -0.3879, -0.1517, -0.0076, -0.2734, -0.1902,\n",
      "         0.3060,  0.4947, -0.2415, -0.4846,  0.5735,  0.7599, -0.5873, -0.9110,\n",
      "         0.0538, -0.7437, -0.3443,  0.0096,  0.6391, -0.1165, -0.3377, -0.0257,\n",
      "         0.1595,  0.0949, -0.0439, -0.3208, -0.4799,  0.3458, -0.3598,  0.0054,\n",
      "        -0.8692, -0.0067, -1.1156, -0.1281,  0.5425,  0.0404,  0.0546, -0.7368,\n",
      "         1.3088, -0.8557,  0.4428,  0.7235, -0.4539, -1.0998, -0.6645,  0.4969,\n",
      "        -0.7037,  0.4230, -1.1653,  1.1786,  0.6105, -0.0166, -0.9500,  0.3372,\n",
      "         0.5717,  0.5302, -1.6760, -0.1979,  0.2240, -0.2421, -0.6272, -0.9270,\n",
      "        -0.4358,  0.0583,  0.1724,  0.3030, -0.0714,  0.2315, -0.6767,  0.7164,\n",
      "        -0.3751, -0.2129, -0.4595, -0.5329, -0.4181,  0.4426,  0.2504,  0.2264],\n",
      "       grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(f\"cls embeddings shape: {cls_embeddings.shape}\")\n",
    "print(f\"cls embeddings: {cls_embeddings}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
