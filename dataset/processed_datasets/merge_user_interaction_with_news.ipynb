{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9bffa79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, load_dataset, DatasetDict, Features, Sequence, Value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2416e3f0",
   "metadata": {},
   "source": [
    "# PolitiFact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6489d952",
   "metadata": {},
   "source": [
    "## Load User Embeddings from Local Disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d73c63c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dataset = load_from_disk(\"./politifact_with_interaction_embeddings_simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cae76f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'interaction_gemini_embeddings', 'interaction_tones'],\n",
      "        num_rows: 381\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'interaction_gemini_embeddings', 'interaction_tones'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(user_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "422db37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split Information:\n",
      "- Split 'train': 381 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'interaction_gemini_embeddings': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'interaction_tones': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
      "- Split 'test': 102 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'interaction_gemini_embeddings': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'interaction_tones': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSplit Information:\")\n",
    "for split_name, split_data in user_dataset.items():\n",
    "    print(f\"- Split '{split_name}': {len(split_data)} examples\")\n",
    "    print(f\"  Features: {split_data.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b51195",
   "metadata": {},
   "source": [
    "# Load News Embeddings from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da815a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 381/381 [00:00<00:00, 9809.58 examples/s]\n",
      "Generating test split: 100%|██████████| 102/102 [00:00<00:00, 7645.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "news_dataset = load_dataset(\"LittleFish-Coder/fake_news_politifact\", cache_dir=\"dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ea8a0d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions'],\n",
      "        num_rows: 381\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(news_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e6d6e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split Information:\n",
      "- Split 'train': 381 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'user_interactions': [{'content': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'tone': Value(dtype='string', id=None)}]}\n",
      "- Split 'test': 102 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'user_interactions': [{'content': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'tone': Value(dtype='string', id=None)}]}\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nSplit Information:\")\n",
    "for split_name, split_data in news_dataset.items():\n",
    "    print(f\"- Split '{split_name}': {len(split_data)} examples\")\n",
    "    print(f\"  Features: {split_data.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ce05a",
   "metadata": {},
   "source": [
    "# Merge User and News Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da88828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding columns from user_dataset to news_dataset...\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "print(\"\\nAdding columns from user_dataset to news_dataset...\")\n",
    "merged_dataset_dict = DatasetDict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "016b5778",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing split: train\n",
      "Processing split: test\n",
      "\n",
      "Columns added. Merged dataset structure (intermediate):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones'],\n",
      "        num_rows: 381\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "for split in news_dataset.keys(): # 遍歷 'train', 'test'\n",
    "    print(f\"Processing split: {split}\")\n",
    "\n",
    "    # Retrieve the columns to be added\n",
    "    interaction_embeddings_col = user_dataset[split]['interaction_gemini_embeddings']\n",
    "    interaction_tones_col = user_dataset[split]['interaction_tones']\n",
    "    \n",
    "    # Create a new dataset with the additional columns\n",
    "    temp_ds = news_dataset[split].add_column(\"interaction_embeddings_list\", interaction_embeddings_col)\n",
    "    temp_ds = temp_ds.add_column(\"interaction_tones\", interaction_tones_col)\n",
    "\n",
    "    merged_dataset_dict[split] = temp_ds\n",
    "\n",
    "print(\"\\nColumns added. Merged dataset structure (intermediate):\")\n",
    "print(merged_dataset_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2d842",
   "metadata": {},
   "source": [
    "# Combine User and News Embeddings as new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7509e608",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def combine_embeddings(example):\n",
    "    roberta_emb = np.array(example['roberta_embeddings'])\n",
    "    interaction_embs = example['interaction_embeddings_list']\n",
    "    \n",
    "    # Ensure interaction embeddings are numpy arrays\n",
    "    interaction_embs_np = [np.array(emb) for emb in interaction_embs if emb is not None and len(emb) > 0]\n",
    "    \n",
    "    if not interaction_embs_np: # Handle cases with no valid interaction embeddings\n",
    "        # Option 1: Use only roberta embedding (or a zero vector of the same shape)\n",
    "        # combined_emb = roberta_emb # Or np.zeros_like(roberta_emb)\n",
    "        # Option 2: For mean calculation, just return roberta_emb\n",
    "        mean_interaction_emb = np.zeros_like(roberta_emb) # Default to zero vector if no interactions\n",
    "    else:\n",
    "        mean_interaction_emb = np.mean(interaction_embs_np, axis=0)\n",
    "        \n",
    "    # Combine by taking the mean of roberta_emb and mean_interaction_emb\n",
    "    # Ensure both are numpy arrays for element-wise mean\n",
    "    combined_emb = np.mean([roberta_emb, mean_interaction_emb], axis=0)\n",
    "    \n",
    "    # Convert back to list for dataset storage if necessary, though numpy array might be fine\n",
    "    example['combined_embeddings'] = combined_emb.tolist() \n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "228c3a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 381/381 [00:03<00:00, 98.84 examples/s] \n",
      "Map: 100%|██████████| 381/381 [00:03<00:00, 98.84 examples/s] \n",
      "Map: 100%|██████████| 102/102 [00:00<00:00, 103.88 examples/s]\n",
      "Map: 100%|██████████| 102/102 [00:00<00:00, 103.88 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Apply the function to each split\n",
    "print(\"\\nCombining embeddings...\")\n",
    "final_dataset_dict = merged_dataset_dict.map(combine_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "957c2c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings combined. Final dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones', 'combined_embeddings'],\n",
      "        num_rows: 381\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones', 'combined_embeddings'],\n",
      "        num_rows: 102\n",
      "    })\n",
      "})\n",
      "\n",
      "Verification (first train example):\n",
      "Roberta Embedding shape: (768,)\n",
      "Interaction Embeddings List length: 20\n",
      "First Interaction Embedding shape: (768,)\n",
      "Combined Embedding shape: (768,)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nEmbeddings combined. Final dataset structure:\")\n",
    "print(final_dataset_dict)\n",
    "\n",
    "# Verify the first example's combined embedding\n",
    "print(\"\\nVerification (first train example):\")\n",
    "print(\"Roberta Embedding shape:\", np.array(final_dataset_dict['train'][0]['roberta_embeddings']).shape)\n",
    "print(\"Interaction Embeddings List length:\", len(final_dataset_dict['train'][0]['interaction_embeddings_list']))\n",
    "if final_dataset_dict['train'][0]['interaction_embeddings_list']:\n",
    "    print(\"First Interaction Embedding shape:\", np.array(final_dataset_dict['train'][0]['interaction_embeddings_list'][0]).shape)\n",
    "print(\"Combined Embedding shape:\", np.array(final_dataset_dict['train'][0]['combined_embeddings']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bb8fb7",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "18b4d2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  2.56ba/s]\n",
      "\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.63s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00,  9.41ba/s]\n",
      "\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.16s/it]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/LittleFish-Coder/Fake_News_PolitiFact/commit/433390806a7fab30ffa53a9a4834648f169b5172', commit_message='Upload dataset', commit_description='', oid='433390806a7fab30ffa53a9a4834648f169b5172', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/LittleFish-Coder/Fake_News_PolitiFact', endpoint='https://huggingface.co', repo_type='dataset', repo_id='LittleFish-Coder/Fake_News_PolitiFact'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# push the dataset to the hub \n",
    "final_dataset_dict.push_to_hub('LittleFish-Coder/fake_news_politifact')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e81eacc",
   "metadata": {},
   "source": [
    "# GossipCop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feec1d",
   "metadata": {},
   "source": [
    "## Load User Embeddings from Local Disk (GossipCop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6f2ef220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'interaction_gemini_embeddings', 'interaction_tones'],\n",
      "        num_rows: 9988\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'interaction_gemini_embeddings', 'interaction_tones'],\n",
      "        num_rows: 2672\n",
      "    })\n",
      "})\n",
      "\n",
      "Split Information:\n",
      "- Split 'train': 9988 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'interaction_gemini_embeddings': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'interaction_tones': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n",
      "- Split 'test': 2672 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'interaction_gemini_embeddings': Sequence(feature=Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), length=-1, id=None), 'interaction_tones': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "user_dataset_gc = load_from_disk(\"./gossipcop_with_interaction_embeddings_simple\")\n",
    "print(user_dataset_gc)\n",
    "print(\"\\nSplit Information:\")\n",
    "for split_name, split_data in user_dataset_gc.items():\n",
    "    print(f\"- Split '{split_name}': {len(split_data)} examples\")\n",
    "    print(f\"  Features: {split_data.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1dca8b",
   "metadata": {},
   "source": [
    "## Load News Embeddings from Hugging Face (GossipCop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c09523dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 9988/9988 [00:00<00:00, 14639.54 examples/s]\n",
      "Generating test split: 100%|██████████| 2672/2672 [00:00<00:00, 16123.70 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions'],\n",
      "        num_rows: 9988\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions'],\n",
      "        num_rows: 2672\n",
      "    })\n",
      "})\n",
      "\n",
      "Split Information:\n",
      "- Split 'train': 9988 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'user_interactions': [{'content': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'tone': Value(dtype='string', id=None)}]}\n",
      "- Split 'test': 2672 examples\n",
      "  Features: {'text': Value(dtype='string', id=None), 'label': Value(dtype='int64', id=None), 'bert_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'roberta_embeddings': Sequence(feature=Value(dtype='float32', id=None), length=-1, id=None), 'user_interactions': [{'content': Value(dtype='string', id=None), 'id': Value(dtype='string', id=None), 'tone': Value(dtype='string', id=None)}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "news_dataset_gc = load_dataset(\"LittleFish-Coder/fake_news_gossipcop\", cache_dir=\"dataset\")\n",
    "print(news_dataset_gc)\n",
    "print(\"\\nSplit Information:\")\n",
    "for split_name, split_data in news_dataset_gc.items():\n",
    "    print(f\"- Split '{split_name}': {len(split_data)} examples\")\n",
    "    print(f\"  Features: {split_data.features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84d2df2",
   "metadata": {},
   "source": [
    "## Merge User and News Embeddings (GossipCop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98885d78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Adding columns from user_dataset_gc to news_dataset_gc...\n",
      "Processing split: train\n",
      "Processing split: test\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nAdding columns from user_dataset_gc to news_dataset_gc...\")\n",
    "merged_dataset_dict_gc = DatasetDict()\n",
    "\n",
    "for split in news_dataset_gc.keys(): # Iterate through 'train', 'test'\n",
    "    print(f\"Processing split: {split}\")\n",
    "\n",
    "    # Retrieve the columns to be added\n",
    "    interaction_embeddings_col_gc = user_dataset_gc[split]['interaction_gemini_embeddings']\n",
    "    interaction_tones_col_gc = user_dataset_gc[split]['interaction_tones']\n",
    "    \n",
    "    # Create a new dataset with the additional columns\n",
    "    temp_ds_gc = news_dataset_gc[split].add_column(\"interaction_embeddings_list\", interaction_embeddings_col_gc)\n",
    "    temp_ds_gc = temp_ds_gc.add_column(\"interaction_tones\", interaction_tones_col_gc)\n",
    "\n",
    "    merged_dataset_dict_gc[split] = temp_ds_gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44cf7c1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Columns added. Merged GossipCop dataset structure (intermediate):\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones'],\n",
      "        num_rows: 9988\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones'],\n",
      "        num_rows: 2672\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nColumns added. Merged GossipCop dataset structure (intermediate):\")\n",
    "print(merged_dataset_dict_gc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b09a9e",
   "metadata": {},
   "source": [
    "## Combine User and News Embeddings as new features (GossipCop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3cf67c92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Combining embeddings for GossipCop...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 9988/9988 [01:44<00:00, 96.01 examples/s] \n",
      "Map: 100%|██████████| 2672/2672 [00:28<00:00, 95.35 examples/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GossipCop embeddings combined. Final dataset structure:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones', 'combined_embeddings'],\n",
      "        num_rows: 9988\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'label', 'bert_embeddings', 'roberta_embeddings', 'user_interactions', 'interaction_embeddings_list', 'interaction_tones', 'combined_embeddings'],\n",
      "        num_rows: 2672\n",
      "    })\n",
      "})\n",
      "\n",
      "Verification (first train example - GossipCop):\n",
      "Roberta Embedding shape: (768,)\n",
      "Interaction Embeddings List length: 20\n",
      "First Interaction Embedding shape: (768,)\n",
      "Combined Embedding shape: (768,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply the combine_embeddings function (defined earlier) to each split\n",
    "print(\"\\nCombining embeddings for GossipCop...\")\n",
    "final_dataset_dict_gc = merged_dataset_dict_gc.map(combine_embeddings)\n",
    "\n",
    "print(\"\\nGossipCop embeddings combined. Final dataset structure:\")\n",
    "print(final_dataset_dict_gc)\n",
    "\n",
    "# Verify the first example's combined embedding\n",
    "print(\"\\nVerification (first train example - GossipCop):\")\n",
    "print(\"Roberta Embedding shape:\", np.array(final_dataset_dict_gc['train'][0]['roberta_embeddings']).shape)\n",
    "print(\"Interaction Embeddings List length:\", len(final_dataset_dict_gc['train'][0]['interaction_embeddings_list']))\n",
    "if final_dataset_dict_gc['train'][0]['interaction_embeddings_list']:\n",
    "    print(\"First Interaction Embedding shape:\", np.array(final_dataset_dict_gc['train'][0]['interaction_embeddings_list'][0]).shape)\n",
    "print(\"Combined Embedding shape:\", np.array(final_dataset_dict_gc['train'][0]['combined_embeddings']).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c64e6f",
   "metadata": {},
   "source": [
    "## Upload to Hugging Face (GossipCop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b4b54243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.30ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:02<00:00,  1.38ba/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 4/4 [00:03<00:00,  1.32ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 3/3 [01:10<00:00, 23.38s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 3/3 [00:02<00:00,  1.19ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:15<00:00, 15.71s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/LittleFish-Coder/Fake_News_GossipCop/commit/e55b0342ad5121b781b8427c3bfc70a930da33cc', commit_message='Upload dataset', commit_description='', oid='e55b0342ad5121b781b8427c3bfc70a930da33cc', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/LittleFish-Coder/Fake_News_GossipCop', endpoint='https://huggingface.co', repo_type='dataset', repo_id='LittleFish-Coder/Fake_News_GossipCop'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset_dict_gc.push_to_hub('LittleFish-Coder/fake_news_gossipcop')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fakenews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
